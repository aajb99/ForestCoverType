0.86, 0.86, 0.86, 0.87, 0.89, 0.91)
n <- length(data1)
#############################################
# POSTERIOR DISTRIBUTIONS: use Gibbs Sampling
#############################################
# Starting values:
# mu <- 0.77
mu <- mean(data1)
# sigma2 <- 0.07^2
sigma2 <- var(data1)
# initializations for the Gibbs Sampling Algorithm
iters <- 10000
mu.save <- rep(0, iters)
mu.save[1] <- mu
sigma2.save <- rep(0, iters)
sigma2.save[1] <- sigma2
#Gibbs Sampling Algorithm
for(t in 2:iters){
# Full conditional of mu (update the value of the parameters)
lambda.p <- (tau2*sum(data1) + sigma2*lambda)/(tau2*n + sigma2)
tau2.p <- sigma2*tau2/(tau2*n + sigma2)
#sample a new value of mu
mu <- rnorm(1, lambda.p, sqrt(tau2.p))
#save the value of mu
mu.save[t] <- mu
# full conditional of sigma2 (update the value of the parameters)
gamma.p <- gamma + n/2
phi.p <- phi + sum((data1 - mu)^2 )/2
#sample new value of sigma2
sigma2 <- rinvgamma(1, gamma.p, phi.p)
#save the value of sigma2
sigma2.save[t] <- sigma2
}
# # Find rolling mean to plot:
# mu.rollmean <- rollmean(mu.save, k = 90, fill = NA)
# # mu.save.sub <- mu.save[seq_along(mu.save) %% 2 != 0]
# mu.save.sub <- mu.save[1:4000]
# # Find rolling mean to plot sigma^2:
# s2.rollmean <- rollmean(sigma2.save, k = 90, fill = NA)
# # mu.save.sub <- mu.save[seq_along(mu.save) %% 2 != 0]
# s2.save.sub <- sigma2.save[1:4000]
# Trace plots (decide if we need to throw out the first few values):
# plot(mu.save.sub, type='l')
# lines(x = mu.rollmean, col = 'light green', type = 'l')
# plot(s2.save.sub, type='l')
# lines(x = s2.rollmean, col = 'light green', type = 'l')
par(mfrow=c(1,2))
plot(mu.save, type='l')
plot(sigma2.save, type='l')
#throw out the first few values
burn <- 200
mu.use <- mu.save[-(1:burn)]
sigma2.use <- sigma2.save[-(1:burn)]
plot(mu.use, type='l')
plot(sigma2.use, type='l')
#SUMMARIZE THE POSTERIOR DISTRIBUTION(S)
# posterior distribution of mu
#plot
plot(density(mu.use), xlab=expression(mu[W]), ylab="density", main=expression(pi(mu[W]~"|"~data)))
# #add prior
# curve(dnorm(x, lambda, sqrt(tau2)), lty=2, add=T)
#add legend
# legend("topright", c("Posterior"))
# posterior distribution of sigma2
par(mfrow=c(1,1))
plot(density(sigma2.use), xlab=expression(sigma[W]^2), main=expression(pi(sigma[W]^2~"|"~data)))
# #add prior
# curve(dinvgamma(x, gamma, phi), add=T, lty=2, xlim=c(0, 1))
#add legend
# legend("topright", c("Posterior"))
library(invgamma)
# PRIOR PARAMETERS: We'll use the same prior for both populations
# How would I change the code to allow for different priors for the two populations?
# Prior parameters for mu:
lambda <- 4 #Bestsellers will have higher reviews
tau2 <- 10 #leave room for uncertainty
# Prior parameters for sigma2:
# possible range for reviews is ~1-5,
# then an approximate standard deviation would be 4/6
# so an approximate variance would be 0.4444
gamma <- 2.01
phi <- 0.4444
phi/(gamma-1)
# Plot the prior distributions to make sure they seem reasonable
par(mfrow=c(1,2))
curve(dnorm(x, lambda, sqrt(tau2)), xlim=c(-4, 12), ylab="prior density", main=expression(pi(mu)), xlab=expression(mu))
curve(dinvgamma(x, gamma, phi), xlim=c(0, 2), ylab="prior density", main=expression(pi(sigma^2)), xlab=expression(sigma^2))
fiction <- c(4.44, 3.84, 4.65, 4.2, 4.18, 4.33, 4.31, 4.32, 3.97, 4.44, 4, 4.34, 4.17, 4.22, 3.84)
n.f <- length(fiction)
nonfiction <- c(4.22, 3.78, 4.15, 4.37, 4.43, 4.56, 4.45, 4.4, 4.42, 4.49, 3.83, 3.85, 4.43, 4.23, 4.55)
n.n <- length(nonfiction)
#Plot data
hist(fiction)
hist(nonfiction)
#qq plot to check for normality
#(not great but not too bad for only 15 observations)
qqnorm(fiction)
qqline(fiction)
qqnorm(nonfiction)
qqline(nonfiction)
mu.f <- 4
sigma2.f <- 0.5
mu.n <- 4
sigma2.n <- 0.5
# initializations for the Gibbs Sampling Algorithm
iters <- 10000
muf.save <- mun.save <- rep(0, iters)
muf.save[1] <- mu.f
mun.save[1] <- mu.n
sigma2f.save <- sigma2n.save <- rep(0, iters)
sigma2f.save[1] <- sigma2.f
sigma2n.save[1] <- sigma2.n
#Gibbs Sampling Algorithm
for(t in 2:iters){
# Full conditional of mu.f (update the value of the parameters)
lambdaf.p <- (tau2*sum(fiction) + sigma2.f*lambda)/(tau2*n.f + sigma2.f)
tau2f.p <- sigma2.f*tau2/(tau2*n.f + sigma2.f)
#sample a new value of mu.f
mu.f <- rnorm(1, lambdaf.p, sqrt(tau2f.p))
#save the value of mu.f
muf.save[t] <- mu.f
#Repeat for mu.n
lambdan.p <- (tau2*sum(nonfiction) + sigma2.n*lambda)/(tau2*n.n + sigma2.n)
tau2n.p <- sigma2.n*tau2/(tau2*n.n + sigma2.n)
mu.n <- rnorm(1, lambdan.p, sqrt(tau2n.p))
mun.save[t] <- mu.n
# full conditional of sigma2 (update the value of the parameters)
gammaf.p <- gamma + n.f/2
phif.p <- phi + sum((fiction - mu.f)^2 )/2
#sample new value of sigma2.f
sigma2.f <- rinvgamma(1, gammaf.p, phif.p)
#save the value of sigma2.f
sigma2f.save[t] <- sigma2.f
#Repeat for sigma2.n
gamman.p <- gamma + n.n/2
phin.p <- phi + sum((nonfiction - mu.n)^2 )/2
sigma2.n <- rinvgamma(1, gamman.p, phin.p)
sigma2n.save[t] <- sigma2.n
}
# Trace plots (decide if we need to throw out the first few values)
par(mfrow=c(1,2))
plot(muf.save, type='l')
plot(sigma2f.save, type='l')
plot(mun.save, type='l')
plot(sigma2n.save, type='l')
#throw out the first few values
burn <- 100
muf.use <- muf.save[-(1:burn)]
sigma2f.use <- sigma2f.save[-(1:burn)]
mun.use <- mun.save[-(1:burn)]
sigma2n.use <- sigma2n.save[-(1:burn)]
plot(muf.use, type='l')
plot(sigma2f.use, type='l')
plot(mun.use, type='l')
plot(sigma2n.use, type='l')
plot(density(muf.use), xlab=expression(mu), ylab="density", main=expression(pi(mu~"|"~data)), lwd=2)
lines(density(mun.use), col='gray', lwd=2)
legend("topleft", c("Fiction", "Nonfiction"), col=c("black", "gray"), lty=1, lwd=2)
# posterior distributions of sigma2.f and sigma2.n
plot(density(sigma2f.use), xlab=expression(sigma^2), ylab="density", main=expression(pi(sigma^2~"|"~data)), lwd=2)
lines(density(sigma2n.use), col='gray', lwd=2)
legend("topright", c("Fiction", "Nonfiction"), col=c("black", "gray"), lty=1, lwd=2)
mean(muf.use > mun.use)
#Given our data and prior knowledge, there is about a 30% chance that
#NYTimes Bestseller fiction books have a higher goodreads rating than nonfiction books
# Credible interval
diff <- muf.use - mun.use
plot(density(diff), xlab=expression(mu[f]-mu[n]), main="Posterior of Average Fiction vs Nonfiction")
abline(v=0, lty=2)
quantile(diff, c(.025, .975))
knitr::opts_chunk$set(echo = TRUE)
library(invgamma)
library(zoo)
# WE:
set.seed(1127)
####################
# # PRIOR PARAMETERS
####################
# # Prior parameters for mu:
lambda <- 0.7
tau2 <- 10^2 #relatively large
# # Prior parameters for sigma2:
gamma <- 2.001
phi <- 0.08
# #prior expected value of variance:
# phi/(gamma-1)
#
# # Plot the prior distributions to make sure they seem reasonable
# par(mfrow=c(1,2))
# curve(dnorm(x, lambda, sqrt(tau2)), xlim=c(-45, 45), ylab="prior density", main=expression(pi(mu)), xlab=expression(mu))
# curve(dinvgamma(x, gamma, phi), xlim=c(0, 1), ylab="prior density", main=expression(pi(sigma^2)), xlab=expression(sigma^2))
##############
# COLLECT DATA
##############
data1 <- c(0.66, 0.66, 0.67, 0.68, 0.68, 0.72, 0.73, 0.73, 0.73, 0.75, 0.75,
0.76, 0.77, 0.77, 0.77, 0.77,0.78, 0.78, 0.78, 0.79, 0.79, 0.79,
0.79, 0.8, 0.8,0.81, 0.82, 0.83, 0.83, 0.83, 0.84, 0.84,0.84, 0.85,
0.86, 0.86, 0.86, 0.87, 0.89, 0.91)
n <- length(data1)
#############################################
# POSTERIOR DISTRIBUTIONS: use Gibbs Sampling
#############################################
# Starting values:
# mu <- 0.77
mu <- mean(data1)
# sigma2 <- 0.07^2
sigma2 <- var(data1)
# initializations for the Gibbs Sampling Algorithm
iters <- 10000
mu.save <- rep(0, iters)
mu.save[1] <- mu
sigma2.save <- rep(0, iters)
sigma2.save[1] <- sigma2
#Gibbs Sampling Algorithm
for(t in 2:iters){
# Full conditional of mu (update the value of the parameters)
lambda.p <- (tau2*sum(data1) + sigma2*lambda)/(tau2*n + sigma2)
tau2.p <- sigma2*tau2/(tau2*n + sigma2)
#sample a new value of mu
mu <- rnorm(1, lambda.p, sqrt(tau2.p))
#save the value of mu
mu.save[t] <- mu
# full conditional of sigma2 (update the value of the parameters)
gamma.p <- gamma + n/2
phi.p <- phi + sum((data1 - mu)^2 )/2
#sample new value of sigma2
sigma2 <- rinvgamma(1, gamma.p, phi.p)
#save the value of sigma2
sigma2.save[t] <- sigma2
}
# # Find rolling mean to plot:
# mu.rollmean <- rollmean(mu.save, k = 90, fill = NA)
# # mu.save.sub <- mu.save[seq_along(mu.save) %% 2 != 0]
# mu.save.sub <- mu.save[1:4000]
# # Find rolling mean to plot sigma^2:
# s2.rollmean <- rollmean(sigma2.save, k = 90, fill = NA)
# # mu.save.sub <- mu.save[seq_along(mu.save) %% 2 != 0]
# s2.save.sub <- sigma2.save[1:4000]
# Trace plots (decide if we need to throw out the first few values):
# plot(mu.save.sub, type='l')
# lines(x = mu.rollmean, col = 'light green', type = 'l')
# plot(s2.save.sub, type='l')
# lines(x = s2.rollmean, col = 'light green', type = 'l')
par(mfrow=c(1,2))
plot(mu.save, type='l')
plot(sigma2.save, type='l')
#throw out the first few values
burn <- 200
mu.use <- mu.save[-(1:burn)]
sigma2.use <- sigma2.save[-(1:burn)]
plot(mu.use, type='l')
plot(sigma2.use, type='l')
#SUMMARIZE THE POSTERIOR DISTRIBUTION(S)
# posterior distribution of mu
#plot
plot(density(mu.use), xlab=expression(mu[W]), ylab="density", main=expression(pi(mu[W]~"|"~data)))
# #add prior
# curve(dnorm(x, lambda, sqrt(tau2)), lty=2, add=T)
#add legend
# legend("topright", c("Posterior"))
# posterior distribution of sigma2
par(mfrow=c(1,1))
plot(density(sigma2.use), xlab=expression(sigma[W]^2), main=expression(pi(sigma[W]^2~"|"~data)))
# #add prior
# curve(dinvgamma(x, gamma, phi), add=T, lty=2, xlim=c(0, 1))
#add legend
# legend("topright", c("Posterior"))
# EA:
# COLLECT DATA
data2 <- c(0.54, 0.57, 0.58, 0.61, 0.62, 0.62, 0.64, 0.64, 0.67, 0.68, 0.68,
0.68, 0.69, 0.69, 0.69, 0.7, 0.7, 0.71, 0.71, 0.72, 0.72, 0.73,
0.74, 0.74, 0.74, 0.74, 0.76, 0.77, 0.77, 0.78, 0.79, 0.8, 0.8,
0.8, 0.82, 0.82, 0.83, 0.85, 0.86, 0.87)
n2 <- length(data2)
# POSTERIOR DISTRIBUTIONS: use Gibbs Sampling
# Starting values:
# mu_ea <- 0.69
mu_ea <- mean(data2)
# sigma2_ea <- 0.07^2
sigma2_ea <- var(data2)
# initializations for the Gibbs Sampling Algorithm
iters <- 10000
mu_ea.save <- rep(0, iters)
mu_ea.save[1] <- mu_ea
sigma2_ea.save <- rep(0, iters)
sigma2_ea.save[1] <- sigma2_ea
#Gibbs Sampling Algorithm
for(t in 2:iters){
# Full conditional of mu (update the value of the parameters)
lambda.p <- (tau2*sum(data2) + sigma2_ea*lambda)/(tau2*n2 + sigma2_ea)
tau2.p <- sigma2_ea*tau2/(tau2*n2 + sigma2_ea)
#sample a new value of mu
mu_ea <- rnorm(1, lambda.p, sqrt(tau2.p))
#save the value of mu
mu_ea.save[t] <- mu_ea
# full conditional of sigma2 (update the value of the parameters)
gamma.p <- gamma + n2/2
phi.p <- phi + sum((data2 - mu_ea)^2 )/2
#sample new value of sigma2
sigma2_ea <- rinvgamma(1, gamma.p, phi.p)
#save the value of sigma2
sigma2_ea.save[t] <- sigma2_ea
}
# Trace plots (decide if we need to throw out the first few values)
par(mfrow=c(1,2))
plot(mu_ea.save, type='l')
plot(sigma2_ea.save, type='l')
#throw out the first few values
burn <- 200
mu_ea.use <- mu_ea.save[-(1:burn)]
sigma2_ea.use <- sigma2_ea.save[-(1:burn)]
plot(mu_ea.use, type='l')
plot(sigma2_ea.use, type='l')
#SUMMARIZE THE POSTERIOR DISTRIBUTION(S)
# posterior distribution of mu
#plot
plot(density(mu_ea.use), xlab=expression(mu[E]), ylab="density", main=expression(pi(mu[E]~"|"~data)))
# #add prior
# curve(dnorm(x, lambda, sqrt(tau2)), lty=2, add=T)
#add legend
#legend("topright", c("Posterior"))
# posterior distribution of sigma2
par(mfrow=c(1,1))
plot(density(sigma2_ea.use), xlab=expression(sigma[E]^2), main=expression(pi(sigma[E]^2~"|"~data)))
# #add prior
# curve(dinvgamma(x, gamma, phi), add=T, lty=2, xlim=c(0, 1))
#add legend
#legend("topright", c("Posterior"))
# Posterior Dist of the difference between mean AUROC for WE and EA:
# Credible interval
diff <- mu.use - mu_ea.use
plot(density(diff), xlab=expression(mu[W]-mu[E]), main=expression(~"Posterior of " ~mu[W] ~" - " ~mu[E]))
abline(v=0, lty=2)
quantile(diff, c(.025, .975))
# Posterior Dist of the difference between variance of AUROC for WE and EA:
# Credible interval
diff <- sigma2.use - sigma2_ea.use
plot(density(diff), xlab=expression(sigma[W]^2-sigma[E]^2), main=expression(~"Posterior of " ~sigma[W]^2 ~" - " ~sigma[E]^2))
abline(v=0, lty=2)
quantile(diff, c(.025, .975))
# WE:
# # PRIOR PARAMETERS
# # Prior parameters for mu:
lambda <- 0.7
tau2 <- 10^2 #relatively large
# # Prior parameters for sigma2:
gamma <- 2.001
phi <- 0.08
# #prior expected value of variance:
# phi/(gamma-1)
#
# # Plot the prior distributions to make sure they seem reasonable
# par(mfrow=c(1,2))
# curve(dnorm(x, lambda, sqrt(tau2)), xlim=c(-45, 45), ylab="prior density", main=expression(pi(mu)), xlab=expression(mu))
# curve(dinvgamma(x, gamma, phi), xlim=c(0, 1), ylab="prior density", main=expression(pi(sigma^2)), xlab=expression(sigma^2))
# COLLECT DATA
data1 <- c( 0.68, 0.71, 0.71, 0.73, 0.74, 0.74, 0.74, 0.75, 0.75, 0.75, 0.76,
0.76, 0.76, 0.76, 0.76, 0.77, 0.77, 0.78, 0.79, 0.79, 0.79, 0.8,
0.82, 0.82, 0.82, 0.83, 0.83, 0.83, 0.83, 0.84, 0.84, 0.84, 0.84,
0.84, 0.85, 0.85, 0.86, 0.87, 0.87, 0.89)
n <- length(data1)
# POSTERIOR DISTRIBUTIONS: use Gibbs Sampling
# Starting values:
mu <- 0.79
sigma2 <- 0.04^2
# initializations for the Gibbs Sampling Algorithm
iters <- 10000
mu.save <- rep(0, iters)
mu.save[1] <- mu
sigma2.save <- rep(0, iters)
sigma2.save[1] <- sigma2
#Gibbs Sampling Algorithm
for(t in 2:iters){
# Full conditional of mu (update the value of the parameters)
lambda.p <- (tau2*sum(data1) + sigma2*lambda)/(tau2*n + sigma2)
tau2.p <- sigma2*tau2/(tau2*n + sigma2)
#sample a new value of mu
mu <- rnorm(1, lambda.p, sqrt(tau2.p))
#save the value of mu
mu.save[t] <- mu
# full conditional of sigma2 (update the value of the parameters)
gamma.p <- gamma + n/2
phi.p <- phi + sum((data1 - mu)^2 )/2
#sample new value of sigma2
sigma2 <- rinvgamma(1, gamma.p, phi.p)
#save the value of sigma2
sigma2.save[t] <- sigma2
}
# Trace plots (decide if we need to throw out the first few values)
par(mfrow=c(1,2))
plot(mu.save, type='l')
plot(sigma2.save, type='l')
#throw out the first few values
burn <- 200
mu.use <- mu.save[-(1:burn)]
sigma2.use <- sigma2.save[-(1:burn)]
plot(mu.use, type='l')
plot(sigma2.use, type='l')
#SUMMARIZE THE POSTERIOR DISTRIBUTION(S)
# posterior distribution of mu
#plot
plot(density(mu.use), xlab=expression(mu), ylab="density", main=expression(pi(mu[W]~"|"~data)))
# #add prior
# curve(dnorm(x, lambda, sqrt(tau2)), lty=2, add=T)
#add legend
# legend("topright", c("Posterior"))
# posterior distribution of sigma2
par(mfrow=c(1,1))
plot(density(sigma2.use), xlab=expression(sigma^2), main=expression(pi(sigma[W]^2~"|"~data)))
# #add prior
# curve(dinvgamma(x, gamma, phi), add=T, lty=2, xlim=c(0, 1))
#add legend
# legend("topright", c("Posterior"))
knitr::opts_chunk$set(echo = TRUE)
library(invgamma)
library(zoo)
shape <- 0.5
rate <- 0.5
#collect data
data <- c(3, 3, 7, 1, 4, 6, 6, 7, 3, 1, 5, 5, 5, 3, 3, 0, 3, 1, 2, 2)
n <- length(data)
n
iters <- 10000 #number of iterations
lambda.save <- rep(0, iters) #initialize a vector to save the accepted values of the parameter
lambda <- 0.5 #starting value of parameter
n.accept <- 0 #tracker: how many times do we accept the proposed value?
s <- 0.3
#MCMC algorithm (metropolis random walk)
for(i in 1:iters){
lambda.dot <- rnorm(1, lambda, s)
if(lambda.dot <= 1 & lambda.dot >=0){
logr <- dpois(data, lambda.dot, log=T) + dgamma(data, shape, rate, log=T) - dpois(data, lambda.dot, log=T) - dgamma(data, shape, rate, log=T)
logu <- log(runif(1))
if(logu <= logr){
lambda <- lambda.dot
n.accept <- n.accept+1
}
}
lambda.save[i] <- lambda
}
lambda.dot
lambda
# Import Dataset:
data_train <- vroom("./data/train.csv") %>%
mutate(Cover_Type=factor(Cover_Type))# grab training data
####### Load Libraries #######
#install.packages('tidyverse')
library(tidyverse)
#install.packages('tidymodels')
library(tidymodels)
#install.packages('DataExplorer')
#install.packages("poissonreg")
# library(poissonreg)
#install.packages("glmnet")
library(glmnet)
#library(patchwork)
# install.packages("rpart")
#install.packages('ranger')
library(ranger)
#install.packages('stacks')
library(stacks)
#install.packages('vroom')
library(vroom)
#install.packages('parsnip')
library(parsnip)
# install.packages('dbarts')
# library(dbarts)
#install.packages('embed')
library(embed)
library(themis)
library(ggplot2)
library(parsnip)
# Import Dataset:
data_train <- vroom("./data/train.csv") %>%
mutate(Cover_Type=factor(Cover_Type))# grab training data
setwd("~/byu_fall_2023/Stat_348/STAT348/ForestCoverType")
# Import Dataset:
data_train <- vroom("./data/train.csv") %>%
mutate(Cover_Type=factor(Cover_Type))# grab training data
ncol(data_train)
# view(data_train)
data_train$Cover_Type
rFormula <- Cover_Type ~ .
## For target encoding/Random Forests: ###
class_rf_recipe <- recipe(rFormula, data = data_train) %>% # set model formula and dataset
step_mutate_at(c(12:55), fn = factor) %>%
#step_other(all_nominal_predictors(), threshold = .001) %>%
step_zv(all_predictors()) %>% # eliminate zero variance predictors
step_lencode_glm(all_nominal_predictors(), outcome = vars(Cover_Type)) #%>% # get hours
#step_pca(all_predictors(), threshold = 0.8) %>% # Threshold between 0 and 1, test run for classification rf
# step_smote(all_outcomes(), neighbors = 5)
prepped_recipe <- prep(class_rf_recipe) # preprocessing new data
baked_data1 <- bake(prepped_recipe, new_data = data_train)
class_rf_mod <- rand_forest(mtry = tune(),
min_n = tune(),
trees = 500) %>% #Type of model
set_engine('ranger') %>%
set_mode('classification')
pretune_workflow <- workflow() %>%
add_recipe(class_rf_recipe) %>%
add_model(class_rf_mod)
## Grid of values to tune over
tuning_grid <- grid_regular(mtry(range = c(2,ncol(baked_data1)-1)),
min_n(),
levels = 3) ## L^2 total tuning possibilities
# Split data for CV
folds <- vfold_cv(data_train, v = 3, repeats = 1)
# Run CV
CV_results <- pretune_workflow %>%
tune_grid(resamples = folds,
grid = tuning_grid,
metrics = metric_set(roc_auc))
