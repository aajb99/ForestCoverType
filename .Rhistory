lambda.dot
lambda
knitr::opts_chunk$set(echo = TRUE)
library(invgamma)
library(zoo)
# Rejection Sampling #
set.seed(1128)
# Proposed k value
k <- .25
#prior parameters
shape <- 0.5
rate <- 0.5
#collect data
data <- c(3, 3, 7, 1, 4, 6, 6, 7, 3, 1, 5, 5, 5, 3, 3, 0, 3, 1, 2, 2)
n <- length(data)
#Rejection sampling setup
iters <- 10000 #number of iterations
lambda.save <- NULL #initialize a variable to save the accepted values of the parameter
# Rejection Sampling
for(i in 1:iters){
lambda.dot <- runif(1)
r <- dpois(data, lambda = lambda.dot, log=T)*dgamma(lambda.dot, shape = shape, rate = rate, log=T)/(k*dunif(lambda.dot, 0, 1))
# Four ways to accept or reject the value of theta.dot with probability r...
## First:
accept <- sample(c(T, F), 1, prob=c(r, 1-r))
if(accept==T){
lambda.save <- c(lambda.save, lambda.dot)}
## Second:
# accept <- rbinom(n=1, size=1, prob=r)
# if(accept==1){theta.save <- c(theta.save, theta.dot)}
## Third:
# u <- runif(1)
# if(u <= r){theta.save <- c(theta.save, theta.dot)}
## Fourth (same as third, but on log scale; best way computationally):
# logr <- dbinom(y, n, theta.dot, log=T) + dbeta(theta.dot, alpha, beta, log=T) - log(k) - dunif(theta.dot, 0, 1, log=T)
# logu <- log(runif(1))
# if(logu <= logr){theta.save <- c(theta.save, theta.dot)}
}
?sample
# Rejection Sampling #
set.seed(1128)
# Proposed k value
k <- .25
#prior parameters
shape <- 0.5
rate <- 0.5
#collect data
data <- c(3, 3, 7, 1, 4, 6, 6, 7, 3, 1, 5, 5, 5, 3, 3, 0, 3, 1, 2, 2)
n <- length(data)
#Rejection sampling setup
iters <- 10000 #number of iterations
lambda.save <- NULL #initialize a variable to save the accepted values of the parameter
# Rejection Sampling
for(i in 1:iters){
lambda.dot <- runif(1)
r <- dpois(data, lambda = lambda.dot, log=T)*dgamma(lambda.dot, shape = shape, rate = rate, log=T)/(k*dunif(lambda.dot, 0, 1))
# Four ways to accept or reject the value of theta.dot with probability r...
## First:
accept <- sample(data, 1, prob=r)
if(accept==T){
lambda.save <- c(lambda.save, lambda.dot)}
## Second:
# accept <- rbinom(n=1, size=1, prob=r)
# if(accept==1){theta.save <- c(theta.save, theta.dot)}
## Third:
# u <- runif(1)
# if(u <= r){theta.save <- c(theta.save, theta.dot)}
## Fourth (same as third, but on log scale; best way computationally):
# logr <- dbinom(y, n, theta.dot, log=T) + dbeta(theta.dot, alpha, beta, log=T) - log(k) - dunif(theta.dot, 0, 1, log=T)
# logu <- log(runif(1))
# if(logu <= logr){theta.save <- c(theta.save, theta.dot)}
}
# Rejection Sampling #
set.seed(1128)
# Proposed k value
k <- .25
#prior parameters
shape <- 0.5
rate <- 0.5
#collect data
data <- c(3, 3, 7, 1, 4, 6, 6, 7, 3, 1, 5, 5, 5, 3, 3, 0, 3, 1, 2, 2)
n <- length(data)
#Rejection sampling setup
iters <- 10000 #number of iterations
lambda.save <- NULL #initialize a variable to save the accepted values of the parameter
# Rejection Sampling
for(i in 1:iters){
lambda.dot <- runif(1)
r <- dpois(data, lambda = lambda.dot, log=T)*dgamma(lambda.dot, shape = shape, rate = rate, log=T)/(k*dunif(lambda.dot, 0, 1))
# Four ways to accept or reject the value of theta.dot with probability r...
## First:
accept <- sample(data, 1, prob=lambda.dot)
if(accept==T){
lambda.save <- c(lambda.save, lambda.dot)}
## Second:
# accept <- rbinom(n=1, size=1, prob=r)
# if(accept==1){theta.save <- c(theta.save, theta.dot)}
## Third:
# u <- runif(1)
# if(u <= r){theta.save <- c(theta.save, theta.dot)}
## Fourth (same as third, but on log scale; best way computationally):
# logr <- dbinom(y, n, theta.dot, log=T) + dbeta(theta.dot, alpha, beta, log=T) - log(k) - dunif(theta.dot, 0, 1, log=T)
# logu <- log(runif(1))
# if(logu <= logr){theta.save <- c(theta.save, theta.dot)}
}
gc()
####### Load Libraries #######
#install.packages('tidyverse')
library(tidyverse)
#install.packages('tidymodels')
library(tidymodels)
#install.packages('DataExplorer')
#install.packages("poissonreg")
# library(poissonreg)
#install.packages("glmnet")
library(glmnet)
#library(patchwork)
# install.packages("rpart")
#install.packages('ranger')
library(ranger)
#install.packages('stacks')
library(stacks)
#install.packages('vroom')
library(vroom)
#install.packages('parsnip')
library(parsnip)
# install.packages('dbarts')
# library(dbarts)
#install.packages('embed')
library(embed)
library(themis)
library(ggplot2)
library(parsnip)
####################
data_train <- vroom("./data/train.csv")
setwd("~/byu_fall_2023/Stat_348/STAT348/ForestCoverType/data")
####### Load Libraries #######
#install.packages('tidyverse')
library(tidyverse)
#install.packages('tidymodels')
library(tidymodels)
#install.packages('DataExplorer')
#install.packages("poissonreg")
# library(poissonreg)
#install.packages("glmnet")
library(glmnet)
#library(patchwork)
# install.packages("rpart")
#install.packages('ranger')
library(ranger)
#install.packages('stacks')
library(stacks)
#install.packages('vroom')
library(vroom)
#install.packages('parsnip')
library(parsnip)
# install.packages('dbarts')
# library(dbarts)
#install.packages('embed')
library(embed)
library(themis)
library(ggplot2)
library(parsnip)
data_train <- vroom("./data/train.csv")
setwd("~/byu_fall_2023/Stat_348/STAT348/ForestCoverType")
data_train <- vroom("./data/train.csv")
view(data_train)
cor_matrix <- cor(data_train)
# Create corr heatmap
heatmap(cor_matrix,
col = colorRampPalette(c('blue', 'white', 'green'))(100),
margins = c(5, 5),
Rowv = NA,
Colv = NA)
gc()
install.packages('bonsai')
install.packages('lightgbm')
# Import Dataset:
data_train <- vroom("./data/train.csv") %>%
mutate(Cover_Type=factor(Cover_Type))# grab training data
setwd("C:/Users/aaron/Documents/byu_fall_2023/Stat_348/STAT348/ForestCoverType")
# Import Dataset:
data_train <- vroom("./data/train.csv") %>%
mutate(Cover_Type=factor(Cover_Type))# grab training data
####### Load Libraries #######
#install.packages('tidyverse')
library(tidyverse)
#install.packages('tidymodels')
library(tidymodels)
#install.packages('DataExplorer')
#install.packages("poissonreg")
# library(poissonreg)
#install.packages("glmnet")
library(glmnet)
#library(patchwork)
# install.packages("rpart")
#install.packages('ranger')
library(ranger)
#install.packages('stacks')
library(stacks)
#install.packages('vroom')
library(vroom)
#install.packages('parsnip')
library(parsnip)
# install.packages('dbarts')
# library(dbarts)
#install.packages('embed')
library(embed)
library(themis)
library(ggplot2)
library(parsnip)
library(bonsai)
library(lightgbm)
# Import Dataset:
data_train <- vroom("./data/train.csv") %>%
mutate(Cover_Type=factor(Cover_Type))# grab training data
rFormula <- Cover_Type ~ .
fct_lgbm_recipe <- recipe(Cover_Type ~ ., data = data_train) %>%
update_role(Id, new_role = "Id") %>%
step_mutate(Id = factor(Id)) %>%
step_mutate_at(all_outcomes(), fn = factor, skip = TRUE)
prepped_recipe_boost <- prep(fct_lgbm_recipe) # preprocessing new data
baked_data_boost <- bake(fct_lgbm_recipe, new_data = data_train)
prepped_recipe_boost <- prep(fct_lgbm_recipe) # preprocessing new data
prepped_recipe_boost
baked_data_boost <- bake(fct_lgbm_recipe, new_data = data_train)
rlang::last_trace()
rFormula <- Cover_Type ~ .
fct_lgbm_recipe <- recipe(Cover_Type ~ ., data = data_train) %>%
step_mutate_at(c(12:55), fn = factor) %>%
step_zv(all_predictors()) %>% # eliminate zero variance predictors
step_nzv(freq_cut = 15070/50) %>%
step_lencode_glm(all_nominal_predictors(), outcome = vars(Cover_Type))
prepped_recipe_boost <- prep(fct_lgbm_recipe) # preprocessing new data
baked_data_boost <- bake(fct_lgbm_recipe, new_data = data_train)
# Import Dataset:
data_train <- vroom("./data/train.csv") %>%
mutate(Cover_Type=factor(Cover_Type))# grab training data
rFormula <- Cover_Type ~ .
fct_lgbm_recipe <- recipe(Cover_Type ~ ., data = data_train) %>%
step_mutate_at(c(12:55), fn = factor) %>%
step_zv(all_predictors()) %>% # eliminate zero variance predictors
step_nzv(freq_cut = 15070/50) %>%
step_lencode_glm(all_nominal_predictors(), outcome = vars(Cover_Type))
prepped_recipe_boost <- prep(fct_lgbm_recipe) # preprocessing new data
baked_data_boost <- bake(fct_lgbm_recipe, new_data = data_train)
rlang::last_trace()
rFormula <- Cover_Type ~ .
fct_lgbm_recipe <- recipe(Cover_Type ~ ., data = data_train) %>%
update_role(Id, new_role = "Id") %>%
step_mutate(Id = factor(Id)) %>%
step_mutate_at(all_outcomes(), fn = factor, skip = TRUE)
prepped_recipe_boost <- prep(fct_lgbm_recipe) # preprocessing new data
baked_data_boost <- bake(prepped_recipe_boost, new_data = data_train)
boost_model <- boost_tree(trees = 500,
tree_depth = 5,
learn_rate = .05,
mtry = 15,
min_n = 12
) %>%
set_engine("lightgbm") %>% #or "xgboost" but lightgbm is faster
set_mode("classification")
boost_wf <- workflow() %>%
add_recipe(fct_lgbm_recipe) %>%
add_model(boost_model)
boost_tuneGrid <- grid_regular(tree_depth(),
trees(),
learn_rate(),
levels=5)
boost_model <- boost_tree(trees = 500,
tree_depth = 5,
learn_rate = .05,
mtry = 15,
min_n = 12
) %>%
set_engine("lightgbm") %>% #or "xgboost" but lightgbm is faster
set_mode("classification")
boost_wf <- workflow() %>%
add_recipe(fct_lgbm_recipe) %>%
add_model(boost_model)
# Split data for CV
folds <- vfold_cv(data_train, v = 5, repeats = 1)
final_wf <- boost_wf %>%
fit(data = data_train)
data_test <- vroom("./data/test.csv") # grab testing data
## CV tune, finalize and predict here and save results22
## This takes a few min (10 on my laptop) so run it on becker if you want
# Kaggle DF
lgbm_predictions_boost <- predict(final_wf,
new_data=data_test,
type="class") %>% # "class" or "prob"
mutate(Id = data_test$Id) %>%
#mutate(ACTION = ifelse(.pred_1 > .95, 1, 0)) %>%
mutate(Cover_Type = .pred_class) %>%
select(Id, Cover_Type)
vroom_write(lgbm_predictions_boost, "./data/lgbm_pred_boost.csv", delim = ",")
install.packages('lightgbm')
install.packages("lightgbm")
gc()
knitr::opts_chunk$set(echo = TRUE)
library(invgamma)
library(zoo)
# Rejection Sampling #
set.seed(1128)
# Proposed k value
k <- .27
#prior parameters
alpha <- 1
beta <- 4
#collect data
y <- 17
n <- 116
#Rejection sampling setup
iters <- 10000 #number of iterations
theta.save <- NULL #initialize a variable to save the accepted values of the parameter
#rejection sampling
for(i in 1:iters){
theta.dot <- runif(1)
r <- dbinom(y, n, theta.dot)*dbeta(theta.dot, alpha, beta)/(k*dunif(theta.dot, 0, 1))
# Four ways to accept or reject the value of theta.dot with probability r...
## First:
accept <- sample(c(T, F), 1, prob=c(r, 1-r))
if(accept==T){
theta.save <- c(theta.save, theta.dot)
}
}
#Plot the approximate posterior distribution
plot(density(theta.save), main=expression(pi(theta~"| data")), xlab=expression(theta), lwd=2)
# Add line for the exact posterior distribution
curve(dbeta(x, y+alpha, beta+n-y), add=T, col='blue', lwd=2, lty=2)
#Can compute posterior summaries as usual with Monte Carlo sampling
mean(theta.save)
sd(theta.save)
quantile(theta.save, c(.025, .975))
#exact posterior summaries since, for this example, we know the exact posterior
(alpha+y)/(alpha+n+beta)
sqrt((alpha+y)*(n-y+beta)/((n+alpha+beta)^2*(n+alpha+beta+1)))
qbeta(c(0.025, .975), alpha+y, n-y+beta)
knitr::opts_chunk$set(echo = TRUE)
library(invgamma)
library(zoo)
# Rejection Sampling #
set.seed(1128)
# Proposed k value
k <- .27
#prior parameters
alpha <- 1
beta <- 4
#collect data
y <- 17
n <- 116
#Rejection sampling setup
iters <- 10000 #number of iterations
theta.save <- NULL #initialize a variable to save the accepted values of the parameter
#rejection sampling
for(i in 1:iters){
theta.dot <- runif(1)
r <- dbinom(y, n, theta.dot)*dbeta(theta.dot, alpha, beta)/(k*dunif(theta.dot, 0, 1))
# Four ways to accept or reject the value of theta.dot with probability r...
## First:
accept <- sample(c(T, F), 1, prob=c(r, 1-r))
if(accept==T){
theta.save <- c(theta.save, theta.dot)
}
}
#Plot the approximate posterior distribution
plot(density(theta.save), main=expression(pi(theta~"| data")), xlab=expression(theta), lwd=2)
# Add line for the exact posterior distribution
curve(dbeta(x, y+alpha, beta+n-y), add=T, col='blue', lwd=2, lty=2)
#Can compute posterior summaries as usual with Monte Carlo sampling
mean(theta.save)
sd(theta.save)
quantile(theta.save, c(.025, .975))
#exact posterior summaries since, for this example, we know the exact posterior
(alpha+y)/(alpha+n+beta)
sqrt((alpha+y)*(n-y+beta)/((n+alpha+beta)^2*(n+alpha+beta+1)))
qbeta(c(0.025, .975), alpha+y, n-y+beta)
# Metropolis RW #
set.seed(1128)
#prior parameters
alpha <- 1
beta <- 4
#collect data
y <- 17
n <- 116
#MCMC setup
iters <- 10000 #number of iterations
theta.save <- rep(0, iters) #initialize a vector to save the accepted values of the parameter
theta <- 0.5 #starting value of parameter
n.accept <- 0 #tracker: how many times do we accept the proposed value?
s <- 0.3 #standard deviation of the proposal distribution
#MCMC algorithm (metropolis random walk)
for(i in 1:iters){
theta.dot <- rnorm(1, theta, s)
if(theta.dot <= 1 & theta.dot >=0){
logr <- dbinom(y, n, theta.dot, log=T) + dbeta(theta.dot, alpha, beta, log=T) - dbinom(y, n, theta, log=T) - dbeta(theta, alpha, beta, log=T)
logu <- log(runif(1))
if(logu <= logr){
theta <- theta.dot
n.accept <- n.accept+1
}
}
theta.save[i] <- theta
}
# Look at a trace plot to assess convergence and determine burn-in
par(mfrow=c(1,2))
plot(theta.save, type='l')
#how often did we accept?
n.accept/iters
burn <- 200
theta.use <- theta.save[-(1:burn)]
plot(theta.use, type='l')
#Plot the approximate posterior distribution
par(mfrow=c(1,1))
plot(density(theta.use), main=expression(pi(theta~"| data")), xlab=expression(theta), lwd=2)
# Add line for the exact posterior distribution
curve(dbeta(x, y+alpha, beta+n-y), add=T, col='blue', lwd=2, lty=2)
#Can compute posterior summaries as usual with Monte Carlo sampling
mean(theta.use)
sd(theta.use)
quantile(theta.use, c(.025, .975))
#Given our data and prior knowledge, there is a
#95% probability that Catoo can identify between
#47% and 88% of individuals with colon cancer
# Rejection Sampling #
set.seed(1128)
# Proposed k value
k <- .25
#prior parameters
shape <- 0.5
rate <- 0.5
#collect data
data <- c(3, 3, 7, 1, 4, 6, 6, 7, 3, 1, 5, 5, 5, 3, 3, 0, 3, 1, 2, 2)
n <- length(data)
#Rejection sampling setup
iters <- 10000 #number of iterations
lambda.save <- NULL #initialize a variable to save the accepted values of the parameter
# Rejection Sampling
for(i in 1:iters){
lambda.dot <- runif(1)
r <- dpois(data, lambda = lambda.dot, log=T)*dgamma(lambda.dot, shape = shape, rate = rate, log=T)/(k*dunif(lambda.dot, 0, 1))
# Four ways to accept or reject the value of theta.dot with probability r...
## First:
accept <- sample(data, 1, prob=r)
if(accept==T){
lambda.save <- c(lambda.save, lambda.dot)}
## Second:
# accept <- rbinom(n=1, size=1, prob=r)
# if(accept==1){theta.save <- c(theta.save, theta.dot)}
## Third:
# u <- runif(1)
# if(u <= r){theta.save <- c(theta.save, theta.dot)}
## Fourth (same as third, but on log scale; best way computationally):
# logr <- dbinom(y, n, theta.dot, log=T) + dbeta(theta.dot, alpha, beta, log=T) - log(k) - dunif(theta.dot, 0, 1, log=T)
# logu <- log(runif(1))
# if(logu <= logr){theta.save <- c(theta.save, theta.dot)}
}
set.seed(1128)
#prior parameters
shape <- 0.5
rate <- 0.5
#collect data
data <- c(3, 3, 7, 1, 4, 6, 6, 7, 3, 1, 5, 5, 5, 3, 3, 0, 3, 1, 2, 2)
n <- length(data)
#MCMC setup
iters <- 10000 #number of iterations
lambda.save <- rep(0, iters) #initialize a vector to save the accepted values of the parameter
lambda <- 0.5 #starting value of parameter
n.accept <- 0 #tracker: how many times do we accept the proposed value?
s <- 0.3 #standard deviation of the proposal distribution
#MCMC algorithm (metropolis random walk)
# MCMC
for(t in 1:iters){
#1. draw a proposal value of alpha from the proposal distribution
lambda.star <- rnorm(1, lambda, s)
if(lambda.star > 0){ #alpha has a lower limit of 0; thus, if alpha.star <= 0, then r=0 => automatically reject
#2. compute r (in this case, log(r)).
logr <- sum(dpois(data, lambda = lambda.star, log=T)) +
dgamma(lambda.star, shape = shape, rate = rate, log=T) -
sum(dpois(data, lambda = lambda, log=T)) -
dgamma(lambda, shape = shape, rate = rate, log=T)
#3. accept or reject alpha.star with prob. r
logu <- log(runif(1))
if(logu <= logr){ #if we randomly accept
lambda <- lambda.star
n.accept <- n.accept + 1
}
}#if alpha.star < 0, automatically reject
#4. Save the new value of alpha
lambda.save[t] <- lambda
}
lambda
# Look at a trace plot to assess convergence and determine burn-in
par(mfrow=c(1,2))
plot(lambda.save, type='l')
#how often did we accept?
n.accept/iters
burn <- 50
lambda.use <- lambda.save[-(1:burn)]
plot(lambda.use, type='l')
#Plot the approximate posterior distribution
par(mfrow=c(1,1))
plot(density(lambda.use), main=expression(pi(lambda~"| data")), xlab=expression(lambda), lwd=2)
# Add line for the exact posterior distribution
lines(density(rgamma(10000, shape + sum(data), rate + n), adjust = 2), add=T, col='blue', lwd=2, lty=2)
#Can compute posterior summaries as usual with Monte Carlo sampling
mean(lambda.use)
sd(lambda.use)
quantile(lambda.use, c(.025, .975))
#Given our data and prior knowledge, there is a
#95% probability that Catoo can identify between
#47% and 88% of individuals with colon cancer
