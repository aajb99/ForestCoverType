beta.start <- 5
#tuning parameters
s.alpha <- 2
s.beta <- 15
#acceptance trackers
accept.alpha <- accept.beta <- 0
# MCMC
for(i in 1:iters){
#sample a value of mu from its full conditional (using Metropolis RW)
alpha.star <- rnorm(1, alpha.start, s.alpha)
#no constraints on mu
logr <- sum(dbeta(y1, alpha.star, beta.start, log = T)) + dgamma(alpha.star, alpha_a, beta_a, log=T) - # likelihood taking our current alpha ratioed by our starting alpha (new/old)
sum(dbeta(y1, alpha.start, beta.start, log = T)) - dgamma(alpha.start, alpha_a, beta_a, log=T)
logu <- log(runif(1))
#print(paste("alpha.start:", alpha.start))
if(logu <= logr){
alpha.start <- alpha.star
accept.alpha <- accept.alpha + 1
}
alpha.save[i] <- alpha.start #save the new current value of alpha
#draw a value of beta from its full conditional distribution (using Metropolis RW)
beta.star <- rnorm(1, beta.start, s.beta)
if(beta.star > 0){ #beta must be > 0
logr <- sum(dbeta(y1, alpha.start, beta.star, log = T)) + dgamma(beta.star, alpha_a, beta_a, log=T) -
sum(dbeta(y1, alpha.start, beta.start, log = T)) - dgamma(beta.start, alpha_a, beta_a, log=T)
logu <- log(runif(1))
if(logu <= logr){
beta.start <- beta.star
accept.beta <- accept.beta + 1
}
}
beta.save[i] <- beta.start #save the new current value of beta
}
#likelihood: Yi|mu, beta iid~ Gumbel(mu, beta) (Note: mu is NOT the mean!)
loglik <- function(yvals, alpha, beta){
ll <- dbeta(yvals, alpha, beta, log = T)
out <- sum(ll) # add log y values of beta dist together
return(ll)
}
#priors (very uninformative -- large uncertainty/variance on both parameters)
alpha_a <- 0.005
beta_a <- 0.005
alpha_b <- 0.005
beta_b <- 0.005
#collect data
y1 <- c(0.0952, 0.0627, 0.0702, 0.0930, 0.0526, 0.0521, 0.1066, 0.0436, 0.0339, 0.1070)
y2 <- c(0.0466, 0.0475, 0.0875, 0.0593, 0.0347, 0.0474, 0.0379, 0.0741, 0.1070, 0.0628)
hist(y1)
hist(y2)
#MCMC setup
#number of iterations
iters <- 10000
#create vectors to save sampled values
alpha.save <- beta.save <- rep(0, iters)
#starting values
alpha.start <- 2
beta.start <- 5
#tuning parameters
s.alpha <- 0.9
s.beta <- 15
#acceptance trackers
accept.alpha <- accept.beta <- 0
# MCMC
for(i in 1:iters){
#sample a value of mu from its full conditional (using Metropolis RW)
alpha.star <- rnorm(1, alpha.start, s.alpha)
#no constraints on mu
logr <- sum(dbeta(y1, alpha.star, beta.start, log = T)) + dgamma(alpha.star, alpha_a, beta_a, log=T) - # likelihood taking our current alpha ratioed by our starting alpha (new/old)
sum(dbeta(y1, alpha.start, beta.start, log = T)) - dgamma(alpha.start, alpha_a, beta_a, log=T)
logu <- log(runif(1))
#print(paste("alpha.start:", alpha.start))
if(logu <= logr){
alpha.start <- alpha.star
accept.alpha <- accept.alpha + 1
}
alpha.save[i] <- alpha.start #save the new current value of alpha
#draw a value of beta from its full conditional distribution (using Metropolis RW)
beta.star <- rnorm(1, beta.start, s.beta)
if(beta.star > 0){ #beta must be > 0
logr <- sum(dbeta(y1, alpha.start, beta.star, log = T)) + dgamma(beta.star, alpha_a, beta_a, log=T) -
sum(dbeta(y1, alpha.start, beta.start, log = T)) - dgamma(beta.start, alpha_a, beta_a, log=T)
logu <- log(runif(1))
if(logu <= logr){
beta.start <- beta.star
accept.beta <- accept.beta + 1
}
}
beta.save[i] <- beta.start #save the new current value of beta
}
#likelihood: Yi|mu, beta iid~ Gumbel(mu, beta) (Note: mu is NOT the mean!)
loglik <- function(yvals, alpha, beta){
ll <- dbeta(yvals, alpha, beta, log = T)
out <- sum(ll) # add log y values of beta dist together
return(ll)
}
#priors (very uninformative -- large uncertainty/variance on both parameters)
alpha_a <- 0.005
beta_a <- 0.005
alpha_b <- 0.005
beta_b <- 0.005
#collect data
y1 <- c(0.0952, 0.0627, 0.0702, 0.0930, 0.0526, 0.0521, 0.1066, 0.0436, 0.0339, 0.1070)
y2 <- c(0.0466, 0.0475, 0.0875, 0.0593, 0.0347, 0.0474, 0.0379, 0.0741, 0.1070, 0.0628)
hist(y1)
hist(y2)
#MCMC setup
#number of iterations
iters <- 10000
#create vectors to save sampled values
alpha.save <- beta.save <- rep(0, iters)
#starting values
alpha.start <- 2
beta.start <- 5
#tuning parameters
s.alpha <- 0.8
s.beta <- 10
#acceptance trackers
accept.alpha <- accept.beta <- 0
# MCMC
for(i in 1:iters){
#sample a value of mu from its full conditional (using Metropolis RW)
alpha.star <- rnorm(1, alpha.start, s.alpha)
#no constraints on mu
logr <- sum(dbeta(y1, alpha.star, beta.start, log = T)) + dgamma(alpha.star, alpha_a, beta_a, log=T) - # likelihood taking our current alpha ratioed by our starting alpha (new/old)
sum(dbeta(y1, alpha.start, beta.start, log = T)) - dgamma(alpha.start, alpha_a, beta_a, log=T)
logu <- log(runif(1))
#print(paste("alpha.start:", alpha.start))
if(logu <= logr){
alpha.start <- alpha.star
accept.alpha <- accept.alpha + 1
}
alpha.save[i] <- alpha.start #save the new current value of alpha
#draw a value of beta from its full conditional distribution (using Metropolis RW)
beta.star <- rnorm(1, beta.start, s.beta)
if(beta.star > 0){ #beta must be > 0
logr <- sum(dbeta(y1, alpha.start, beta.star, log = T)) + dgamma(beta.star, alpha_a, beta_a, log=T) -
sum(dbeta(y1, alpha.start, beta.start, log = T)) - dgamma(beta.start, alpha_a, beta_a, log=T)
logu <- log(runif(1))
if(logu <= logr){
beta.start <- beta.star
accept.beta <- accept.beta + 1
}
}
beta.save[i] <- beta.start #save the new current value of beta
}
#likelihood: Yi|mu, beta iid~ Gumbel(mu, beta) (Note: mu is NOT the mean!)
loglik <- function(yvals, alpha, beta){
ll <- dbeta(yvals, alpha, beta, log = T)
out <- sum(ll) # add log y values of beta dist together
return(ll)
}
#priors (very uninformative -- large uncertainty/variance on both parameters)
alpha_a <- 0.005
beta_a <- 0.005
alpha_b <- 0.005
beta_b <- 0.005
#collect data
y1 <- c(0.0952, 0.0627, 0.0702, 0.0930, 0.0526, 0.0521, 0.1066, 0.0436, 0.0339, 0.1070)
y2 <- c(0.0466, 0.0475, 0.0875, 0.0593, 0.0347, 0.0474, 0.0379, 0.0741, 0.1070, 0.0628)
hist(y1)
hist(y2)
#MCMC setup
#number of iterations
iters <- 10000
#create vectors to save sampled values
alpha.save <- beta.save <- rep(0, iters)
#starting values
alpha.start <- 2
beta.start <- 5
#tuning parameters
s.alpha <- 0.5
s.beta <- 10
#acceptance trackers
accept.alpha <- accept.beta <- 0
# MCMC
for(i in 1:iters){
#sample a value of mu from its full conditional (using Metropolis RW)
alpha.star <- rnorm(1, alpha.start, s.alpha)
#no constraints on mu
logr <- sum(dbeta(y1, alpha.star, beta.start, log = T)) + dgamma(alpha.star, alpha_a, beta_a, log=T) - # likelihood taking our current alpha ratioed by our starting alpha (new/old)
sum(dbeta(y1, alpha.start, beta.start, log = T)) - dgamma(alpha.start, alpha_a, beta_a, log=T)
logu <- log(runif(1))
#print(paste("alpha.start:", alpha.start))
if(logu <= logr){
alpha.start <- alpha.star
accept.alpha <- accept.alpha + 1
}
alpha.save[i] <- alpha.start #save the new current value of alpha
#draw a value of beta from its full conditional distribution (using Metropolis RW)
beta.star <- rnorm(1, beta.start, s.beta)
if(beta.star > 0){ #beta must be > 0
logr <- sum(dbeta(y1, alpha.start, beta.star, log = T)) + dgamma(beta.star, alpha_a, beta_a, log=T) -
sum(dbeta(y1, alpha.start, beta.start, log = T)) - dgamma(beta.start, alpha_a, beta_a, log=T)
logu <- log(runif(1))
if(logu <= logr){
beta.start <- beta.star
accept.beta <- accept.beta + 1
}
}
beta.save[i] <- beta.start #save the new current value of beta
}
#check acceptance rates
accept.alpha/iters
accept.beta/iters
#Look at trace plots
plot(alpha.save, type='l')
plot(beta.save, type='l')
burn <- 1000
alpha.use <- alpha.save[-(1:burn)]
beta.use <- beta.save[-(1:burn)]
plot(alpha.use, beta.use) #are these parameters correlated a posteriori?
#likelihood: Yi|mu, beta iid~ Gumbel(mu, beta) (Note: mu is NOT the mean!)
loglik <- function(yvals, alpha, beta){
ll <- dbeta(yvals, alpha, beta, log = T)
out <- sum(ll) # add log y values of beta dist together
return(ll)
}
#priors (very uninformative -- large uncertainty/variance on both parameters)
alpha_a <- 0.005
beta_a <- 0.005
alpha_b <- 0.005
beta_b <- 0.005
hist(y2)
hist(y2)
#MCMC setup
#number of iterations
iters <- 10000
#create vectors to save sampled values
alpha.save <- beta.save <- rep(0, iters)
#starting values
alpha.start <- 2
beta.start <- 5
#tuning parameters
s.alpha <- 0.5
s.beta <- 10
#acceptance trackers
accept.alpha <- accept.beta <- 0
# MCMC
for(i in 1:iters){
#sample a value of mu from its full conditional (using Metropolis RW)
alpha.star <- rnorm(1, alpha.start, s.alpha)
#no constraints on mu
logr <- sum(dbeta(y2, alpha.star, beta.start, log = T)) + dgamma(alpha.star, alpha_a, beta_a, log=T) - # likelihood taking our current alpha ratioed by our starting alpha (new/old)
sum(dbeta(y2, alpha.start, beta.start, log = T)) - dgamma(alpha.start, alpha_a, beta_a, log=T)
logu <- log(runif(1))
#print(paste("alpha.start:", alpha.start))
if(logu <= logr){
alpha.start <- alpha.star
accept.alpha <- accept.alpha + 1
}
alpha.save[i] <- alpha.start #save the new current value of alpha
#draw a value of beta from its full conditional distribution (using Metropolis RW)
beta.star <- rnorm(1, beta.start, s.beta)
if(beta.star > 0){ #beta must be > 0
logr <- sum(dbeta(y2, alpha.start, beta.star, log = T)) + dgamma(beta.star, alpha_a, beta_a, log=T) -
sum(dbeta(y2, alpha.start, beta.start, log = T)) - dgamma(beta.start, alpha_a, beta_a, log=T)
logu <- log(runif(1))
if(logu <= logr){
beta.start <- beta.star
accept.beta <- accept.beta + 1
}
}
beta.save[i] <- beta.start #save the new current value of beta
}
gc()
setwd("~/byu_fall_2023/Stat_348/STAT348/ForestCoverType")
####### Load Libraries #######
#install.packages('tidyverse')
library(tidyverse)
#install.packages('tidymodels')
library(tidymodels)
#install.packages('DataExplorer')
#install.packages("poissonreg")
# library(poissonreg)
#install.packages("glmnet")
library(glmnet)
#library(patchwork)
# install.packages("rpart")
#install.packages('ranger')
library(ranger)
#install.packages('stacks')
library(stacks)
#install.packages('vroom')
library(vroom)
#install.packages('parsnip')
library(parsnip)
# install.packages('dbarts')
# library(dbarts)
#install.packages('embed')
library(embed)
library(themis)
library(ggplot2)
library(parsnip)
library(bonsai)
library(lightgbm)
library(keras)
install.packages('baguette')
library(baguette)
install.packages('stacks')
library(stacks)
################
install.packages("stacks")
data_train <- vroom("./data/train.csv") %>%
mutate(Cover_Type=factor(Cover_Type))# grab training data
data_test <- vroom("./data/test.csv") # grab testing data
rFormula <- Cover_Type ~ .
# fct_recipe <- recipe(Cover_Type ~ ., data = data_train) %>%
#   update_role(Id, new_role = "Id") %>%
#   # step_mutate_at(c(12:55), fn = factor) %>%
#   step_nzv(freq_cut = 15070/50) %>%
#   step_zv(all_predictors()) %>%
#   step_lencode_glm(all_nominal_predictors(), outcome = vars(Cover_Type)) #%>%
#   #step_normalize(all_numeric_predictors())
# fct_recipe <- recipe(Cover_Type ~ ., data = data_train) %>%
#   update_role(Id, new_role = "Id") %>%
#   step_mutate(Id = factor(Id)) %>%
#   step_mutate_at(all_outcomes(), fn = factor, skip = TRUE) %>%
#   step_zv(all_predictors()) #%>%
#   #step_lencode_glm(all_nominal_predictors(), outcome = vars(Cover_Type))
fct_recipe <- recipe(rFormula, data = data_train) %>%
step_zv(all_predictors()) %>%
step_normalize(all_numeric_predictors())
prepped_recipe <- prep(fct_recipe) # preprocessing new data
baked_data <- bake(prepped_recipe, new_data = data_train)
untuned_model <- control_stack_grid()
tuned_model <- control_stack_resamples()
folds <- vfold_cv(data_train, v = 5, repeats = 1)
# Model 1: Classification RF
class_rf_mod <- rand_forest(mtry = tune(),
min_n = tune(),
trees = 500) %>% #Type of model
set_engine('ranger') %>%
set_mode('classification')
rf_pretune_wf <- workflow() %>%
add_recipe(fct_recipe) %>%
add_model(class_rf_mod)
## Grid of values to tune over
tuning_grid <- grid_regular(mtry(range = c(2,ncol(data_train)-1)),
min_n(),
levels = 3) ## L^2 total tuning possibilities
# Run CV
rf_final_mod <- rf_pretune_wf %>%
tune_grid(resamples = folds,
grid = tuning_grid,
metrics = metric_set(roc_auc))
bestTune <- rf_final_mod %>%
select_best('roc_auc')
bestTune
rf_results1 <- rf_pretune_wf %>%
finalize_workflow(bestTune) %>%
fit(data = data_train)
rf_results1
xgboost_recipe <- recipe(rFormula, data = data_train) %>%
step_zv(all_predictors()) %>%
step_normalize(all_numeric_predictors())
boost_model <- boost_tree(trees = 500,
tree_depth = 8,
learn_rate = .2
) %>%
set_engine("xgboost") %>% #or "xgboost" but lightgbm is faster
set_mode("classification")
boost_wf <- workflow() %>%
add_recipe(xgboost_recipe) %>%
add_model(boost_model)
boost_results1 <- fit_resamples(boost_wf,
resamples = folds,
metrics = metric_set(roc_auc),
control = tuned_model)
boost_results1
rf_results1
boost_results1
nn_recipe <- recipe(Cover_Type~., data = data_train) %>%
step_rm(Id) %>%
step_zv(all_numeric_predictors()) %>%
step_range(all_numeric_predictors(), min=0, max=1)
nn_model <- mlp(hidden_units = tune(),
epochs = 50) %>%
set_engine('nnet') %>%
set_mode('classification')
nn_wf <- workflow %>%
add_recipe(nn_recipe) %>%
add_model(nn_model)
nn_recipe
?nn_recipe
nn_wf <- workflow %>%
add_recipe(nn_recipe) %>%
add_model(nn_model)
rlang::last_trace()
typeof(nn_recipe)
nn_wf <- workflow %>%
add_recipe(nn_recipe) %>%
add_model(nn_model)
nn_wf <- workflow() %>%
add_recipe(nn_recipe) %>%
add_model(nn_model)
nn_tuneGrid <- grid_regular(hidden_units(range=c(1,10)),
levels=5)
# Run CV
tuned_nn <- nn_wf %>%
tune_grid(resamples = folds,
grid = nn_tuneGrid,
metrics = metric_set(roc_auc))
bestTune_nn <- tuned_nn %>%
select_best('roc_auc')
bestTune_nn
nn_results1 <- nn_wf %>%
finalize_workflow(bestTune_nn) %>%
fit(data = data_train)
# vroom_write(lgbm_predictions_boost, "./data/lgbm_pred_boost.csv", delim = ",")
save(file = 'RFFinal.RData', list = c('rf_results1'))
save(file = 'BoostFinal.RData', list = c('boost_results1'))
save(file = 'NNFinal.RData', list = c('nn_results1 '))
save(file = 'NNFinal.RData', list = c('nn_results1'))
library(glmnet)
#library(patchwork)
# install.packages("rpart")
#install.packages('ranger')
library(ranger)
#install.packages('stacks')
library(stacks)
#install.packages('vroom')
library(vroom)
#install.packages('parsnip')
library(parsnip)
# install.packages('dbarts')
# library(dbarts)
#install.packages('embed')
library(embed)
library(themis)
library(ggplot2)
library(parsnip)
library(bonsai)
library(lightgbm)
library(keras)
# install.packages('baguette')
library(baguette)
# install.packages('stacks')
library(stacks)
load('RFFinal.RData')
load('BoostFinal.RData')
load('NNFinal.RData')
# Import Dataset:
data_train <- vroom("./data/train.csv") %>%
mutate(Cover_Type=factor(Cover_Type))# grab training data
data_test <- vroom("./data/test.csv") # grab testing data
models_stack <-
stacks() %>%
add_candidates(rf_results1) %>%
add_candidates(boost_results1) %>%
add_candidates(nn_results1)
gc()
typeof(rf_results1)
rf_results1
save(file = 'RFFinal.RData', list = c('rf_final_mod'))
save(file = 'BoostFinal.RData', list = c('boost_results1'))
save(file = 'NNFinal.RData', list = c('tuned_nn'))
load('RFFinal.RData')
load('BoostFinal.RData')
load('NNFinal.RData')
rf_results <- pull_workflow_results(rf_final_mod)
models_stack <-
stacks() %>%
add_candidates(rf_final_mod) %>%
add_candidates(boost_results1) %>%
add_candidates(tuned_nn)
rf_results <- control_stack(resamples = rf_final_mod)
# install.packages('stacks')
library(stacks)
rf_results <- control_stack(resamples = rf_final_mod)
library(tune)
rf_results <- control_stack(resamples = rf_final_mod)
rf_results <- control_stack_resamples(resamples = rf_final_mod)
?control_stack_resamples
rf_results <- control_stack_resamples(rf_final_mod)
rf_results <- rf_final_mod %>%
control_stack(
method = "cv",  # Specify the resampling method (cross-validation in this case)
number = 5  # Specify the number of folds or resamples
# Add any other relevant control settings
)
rf_results <- rf_final_mod %>%
set_resamples(
method = "cv",  # Specify the resampling method (cross-validation in this case)
number = 5  # Specify the number of folds or resamples
# Add any other relevant control settings
)
fct_rf_preds <- predict(rf_results1,
new_data=data_test,
type="class") %>% # "class" or "prob"
mutate(Id = data_test$Id) %>%
#mutate(ACTION = ifelse(.pred_1 > .95, 1, 0)) %>%
mutate(Cover_Type = .pred_class) %>%
select(Id, Cover_Type)
vroom_write(fct_rf_preds, "./data/fct_rf_preds3.csv", delim = ",")
fct_nn_preds <- predict(nn_results1,
new_data=data_test,
type="class") %>% # "class" or "prob"
mutate(Id = data_test$Id) %>%
#mutate(ACTION = ifelse(.pred_1 > .95, 1, 0)) %>%
mutate(Cover_Type = .pred_class) %>%
select(Id, Cover_Type)
vroom_write(fct_nn_preds, "./data/fct_nn_preds.csv", delim = ",")
fct_boost_preds <- predict(boost_results1,
new_data=data_test,
type="class") %>% # "class" or "prob"
mutate(Id = data_test$Id) %>%
#mutate(ACTION = ifelse(.pred_1 > .95, 1, 0)) %>%
mutate(Cover_Type = .pred_class) %>%
select(Id, Cover_Type)
boost_results1
?control_stack
gc()
