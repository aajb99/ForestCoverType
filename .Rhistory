lambda
knitr::opts_chunk$set(echo = TRUE)
library(invgamma)
library(zoo)
# Rejection Sampling #
set.seed(1128)
# Proposed k value
k <- .25
#prior parameters
shape <- 0.5
rate <- 0.5
#collect data
data <- c(3, 3, 7, 1, 4, 6, 6, 7, 3, 1, 5, 5, 5, 3, 3, 0, 3, 1, 2, 2)
n <- length(data)
#Rejection sampling setup
iters <- 10000 #number of iterations
lambda.save <- NULL #initialize a variable to save the accepted values of the parameter
# Rejection Sampling
for(i in 1:iters){
lambda.dot <- runif(1)
r <- dpois(data, lambda = lambda.dot, log=T)*dgamma(lambda.dot, shape = shape, rate = rate, log=T)/(k*dunif(lambda.dot, 0, 1))
# Four ways to accept or reject the value of theta.dot with probability r...
## First:
accept <- sample(c(T, F), 1, prob=c(r, 1-r))
if(accept==T){
lambda.save <- c(lambda.save, lambda.dot)}
## Second:
# accept <- rbinom(n=1, size=1, prob=r)
# if(accept==1){theta.save <- c(theta.save, theta.dot)}
## Third:
# u <- runif(1)
# if(u <= r){theta.save <- c(theta.save, theta.dot)}
## Fourth (same as third, but on log scale; best way computationally):
# logr <- dbinom(y, n, theta.dot, log=T) + dbeta(theta.dot, alpha, beta, log=T) - log(k) - dunif(theta.dot, 0, 1, log=T)
# logu <- log(runif(1))
# if(logu <= logr){theta.save <- c(theta.save, theta.dot)}
}
?sample
# Rejection Sampling #
set.seed(1128)
# Proposed k value
k <- .25
#prior parameters
shape <- 0.5
rate <- 0.5
#collect data
data <- c(3, 3, 7, 1, 4, 6, 6, 7, 3, 1, 5, 5, 5, 3, 3, 0, 3, 1, 2, 2)
n <- length(data)
#Rejection sampling setup
iters <- 10000 #number of iterations
lambda.save <- NULL #initialize a variable to save the accepted values of the parameter
# Rejection Sampling
for(i in 1:iters){
lambda.dot <- runif(1)
r <- dpois(data, lambda = lambda.dot, log=T)*dgamma(lambda.dot, shape = shape, rate = rate, log=T)/(k*dunif(lambda.dot, 0, 1))
# Four ways to accept or reject the value of theta.dot with probability r...
## First:
accept <- sample(data, 1, prob=r)
if(accept==T){
lambda.save <- c(lambda.save, lambda.dot)}
## Second:
# accept <- rbinom(n=1, size=1, prob=r)
# if(accept==1){theta.save <- c(theta.save, theta.dot)}
## Third:
# u <- runif(1)
# if(u <= r){theta.save <- c(theta.save, theta.dot)}
## Fourth (same as third, but on log scale; best way computationally):
# logr <- dbinom(y, n, theta.dot, log=T) + dbeta(theta.dot, alpha, beta, log=T) - log(k) - dunif(theta.dot, 0, 1, log=T)
# logu <- log(runif(1))
# if(logu <= logr){theta.save <- c(theta.save, theta.dot)}
}
# Rejection Sampling #
set.seed(1128)
# Proposed k value
k <- .25
#prior parameters
shape <- 0.5
rate <- 0.5
#collect data
data <- c(3, 3, 7, 1, 4, 6, 6, 7, 3, 1, 5, 5, 5, 3, 3, 0, 3, 1, 2, 2)
n <- length(data)
#Rejection sampling setup
iters <- 10000 #number of iterations
lambda.save <- NULL #initialize a variable to save the accepted values of the parameter
# Rejection Sampling
for(i in 1:iters){
lambda.dot <- runif(1)
r <- dpois(data, lambda = lambda.dot, log=T)*dgamma(lambda.dot, shape = shape, rate = rate, log=T)/(k*dunif(lambda.dot, 0, 1))
# Four ways to accept or reject the value of theta.dot with probability r...
## First:
accept <- sample(data, 1, prob=lambda.dot)
if(accept==T){
lambda.save <- c(lambda.save, lambda.dot)}
## Second:
# accept <- rbinom(n=1, size=1, prob=r)
# if(accept==1){theta.save <- c(theta.save, theta.dot)}
## Third:
# u <- runif(1)
# if(u <= r){theta.save <- c(theta.save, theta.dot)}
## Fourth (same as third, but on log scale; best way computationally):
# logr <- dbinom(y, n, theta.dot, log=T) + dbeta(theta.dot, alpha, beta, log=T) - log(k) - dunif(theta.dot, 0, 1, log=T)
# logu <- log(runif(1))
# if(logu <= logr){theta.save <- c(theta.save, theta.dot)}
}
gc()
setwd("~/byu_fall_2023/Stat_348/STAT348/ForestCoverType")
####### Load Libraries #######
#install.packages('tidyverse')
library(tidyverse)
#install.packages('tidymodels')
library(tidymodels)
#install.packages('DataExplorer')
#install.packages("poissonreg")
# library(poissonreg)
#install.packages("glmnet")
library(glmnet)
#library(patchwork)
# install.packages("rpart")
#install.packages('ranger')
library(ranger)
#install.packages('stacks')
library(stacks)
#install.packages('vroom')
library(vroom)
#install.packages('parsnip')
library(parsnip)
# install.packages('dbarts')
# library(dbarts)
#install.packages('embed')
library(embed)
library(themis)
library(ggplot2)
library(parsnip)
# Import Dataset:
data_train <- vroom("./data/train.csv") %>%
mutate(Cover_Type=factor(Cover_Type))# grab training data
rFormula <- Cover_Type ~ .
# Import Dataset:
data_train <- vroom("./data/train.csv") %>%
mutate(Cover_Type=factor(Cover_Type))# grab training data
data_train <- vroom("./data/train.csv")
cor_matrix <- cor(data_train)
data_train
####### Load Libraries #######
#install.packages('tidyverse')
library(tidyverse)
#install.packages('tidymodels')
library(tidymodels)
#install.packages('DataExplorer')
#install.packages("poissonreg")
# library(poissonreg)
#install.packages("glmnet")
library(glmnet)
#library(patchwork)
# install.packages("rpart")
#install.packages('ranger')
library(ranger)
#install.packages('stacks')
library(stacks)
#install.packages('vroom')
library(vroom)
#install.packages('parsnip')
library(parsnip)
# install.packages('dbarts')
# library(dbarts)
#install.packages('embed')
library(embed)
library(themis)
library(ggplot2)
library(parsnip)
cor_matrix <- cor(data_train)
# Create corr heatmap
heatmap(cor_matrix,
col = colorRampPalette(c('blue', 'white', 'green'))(100),
margins = c(5, 5),
Rowv = NA,
Colv = NA)
?filter
?filter
gc()
view(data_train)
data_wild_areas <- data_train %>%
select(c('Wilderness_Area1',
'Wilderness_Area2',
'Wilderness_Area3',
'Wilderness_Area4'))
data_wild_areas
cor_matrix <- cor(data_wild_areas)
cor_matrix_wild_areas <- cor(data_wild_areas)
# Create corr heatmap
heatmap(cor_matrix_wild_areas,
col = colorRampPalette(c('blue', 'white', 'green'))(100),
margins = c(5, 5),
Rowv = NA,
Colv = NA)
# Create corr heatmap
heatmap(cor_matrix_wild_areas,
col = colorRampPalette(c('blue', 'white', 'green'))(100),
margins = c(5, 5),
Rowv = NA,
Colv = NA)
# Create corr heatmap
heatmap(cor_matrix_wild_areas,
col = colorRampPalette(c('blue', 'white', 'green'))(100),
margins = c(5, 5),
Rowv = NA,
Colv = NA)
# Create corr heatmap
heatmap(cor_matrix_wild_areas,
col = colorRampPalette(c('blue', 'white', 'green'))(100),
margins = c(5, 5),
Rowv = NA,
Colv = NA)
# Create corr heatmap
heatmap(cor_matrix_wild_areas,
col = colorRampPalette(c('blue', 'white', 'green'))(100),
margins = c(5, 5),
Rowv = NA,
Colv = NA,
cexRow = 0.8,
cexCol = 0.8)
# Create corr heatmap
heatmap(cor_matrix_wild_areas,
col = colorRampPalette(c('blue', 'white', 'green'))(100),
margins = c(5, 5),
Rowv = NA,
Colv = NA,
cexRow = 0.6,
cexCol = 0.6)
# Create corr heatmap
heatmap(cor_matrix_wild_areas,
col = colorRampPalette(c('blue', 'white', 'green'))(100),
margins = c(5, 5),
Rowv = NA,
Colv = NA,
cexRow = 0.6,
cexCol = 0.6,
legend = rownames(cor_matrix_wild_areas),  # Use rownames as labels for the color bar
ColSideColors = colorRampPalette(c('blue', 'white', 'green'))(100))
# Create corr heatmap
heatmap(cor_matrix_wild_areas,
col = colorRampPalette(c('blue', 'white', 'green'))(100),
margins = c(5, 5),
Rowv = NA,
Colv = NA,
cexRow = 0.6,
cexCol = 0.6,
legend = rownames(cor_matrix_wild_areas))
# Create corr heatmap
heatmap(cor_matrix_wild_areas,
col = colorRampPalette(c('blue', 'white', 'green'))(100),
margins = c(5, 5),
Rowv = NA,
Colv = NA,
cexRow = 0.6,
cexCol = 0.6)
cols(data_train)
col(data_train)
colnames(data_train)
unique(data_train$Slope)
column <- NA
data_train['Id']
column <- NA
unique_count <- 0
for(column in 1:length(colnames(data_train))){
new_uniq <- NA
val <- 0
for(val in 1:length(column)){
if(new_uniq != data_train[column][val]) {
unique_count <- unique_count + 1
}
}
}
colnames(data_train)
column_unique_df <- data.frame(colnames(data_train))
column_unique_df
column_unique_df$frequency <- NA
column_unique_df
data_train['Id']
column_unique_df <- data.frame(colnames(data_train))
column_unique_df$frequency <- NA
unique_count <- 0
for(column in colnames(data_train)){
unique_vals <- unique(data_train[[column]])
for(i in seq_along(data_train[[colnames.data_train.]])){
if (data_train[[colnames.data_train.]][i] == column){
column_unique_df[['frequency']][i] == unique_vals
}
}
}
unique_count <- 0
for(column in colnames(data_train)){
unique_vals <- unique(data_train[[column]])
for(i in seq_along(data_train[['colnames.data_train.']])){
if (data_train[['colnames.data_train.']][i] == column){
column_unique_df[['frequency']][i] == unique_vals
}
}
}
column_unique_df
for(column in colnames(data_train)){
unique_vals <- unique(data_train[[column]])
for(i in seq_along(data_train[['colnames.data_train.']])){
if (data_train[['colnames.data_train.']][i] == column){
column_unique_df[['frequency']][i] <- unique_vals
}
}
}
column_unique_df
column_unique_df <- data.frame(column_name = character(0), frequency = list())
column_unique_df <- data.frame(column_name = character(0), frequency = list())
for(column in colnames(data_train)){
unique_vals <- unique(data_train[[column]])
new_row <- data.frame(column_name = column, frequency = list(unique_vals))
column_unique_df <- rbind(column_unique_df, new_row)
# for(i in seq_along(data_train[['colnames.data_train.']])){
#   if (data_train[['colnames.data_train.']][i] == column){
#     column_unique_df[['frequency']][i] <- unique_vals
#   }
# }
}
for(column in colnames(data_train)){
unique_vals <- unique(data_train[[column]])
new_row <- data.frame(column_name = column, frequency = list(unique_vals))
rownames(new_row) <- NULL
column_unique_df <- rbind(column_unique_df, new_row)
# for(i in seq_along(data_train[['colnames.data_train.']])){
#   if (data_train[['colnames.data_train.']][i] == column){
#     column_unique_df[['frequency']][i] <- unique_vals
#   }
# }
}
column_unique_df <- data.frame(column_name = character(0), frequency = list())
for(column in colnames(data_train)){
unique_vals <- unique(data_train[[column]])
new_row <- data.frame(column_name = column, frequency = list(unique_vals))
rownames(new_row) <- NULL
column_unique_df <- rbind(column_unique_df, new_row)
# for(i in seq_along(data_train[['colnames.data_train.']])){
#   if (data_train[['colnames.data_train.']][i] == column){
#     column_unique_df[['frequency']][i] <- unique_vals
#   }
# }
}
column_unique_df <- data.frame(column_name = character(0), frequency = list())
for(column in colnames(data_train)){
unique_vals <- unique(data_train[[column]])
new_row <- data.frame(column_name = column, frequency = list(unique_vals))
colnames(new_row) <- colnames(column_unique_df)
column_unique_df <- rbind(column_unique_df, new_row)
# for(i in seq_along(data_train[['colnames.data_train.']])){
#   if (data_train[['colnames.data_train.']][i] == column){
#     column_unique_df[['frequency']][i] <- unique_vals
#   }
# }
}
column_unique_df
unique(data_train[['Id']])
count(unique(data_train[['Id']]))
length(unique(data_train[['Id']]))
column_unique_df <- data.frame(column_name = character(0), frequency = list())
for(column in colnames(data_train)){
unique_vals <- length(unique(data_train[[column]]))
new_row <- data.frame(column_name = column, frequency = list(unique_vals))
colnames(new_row) <- colnames(column_unique_df)
column_unique_df <- rbind(column_unique_df, new_row)
# for(i in seq_along(data_train[['colnames.data_train.']])){
#   if (data_train[['colnames.data_train.']][i] == column){
#     column_unique_df[['frequency']][i] <- unique_vals
#   }
# }
}
column_unique_df
table(data_train[['Wilderness_Area1']])
table(data_train[['Wilderness_Area2']])
table(data_train[['Soil_Type1']])
table(data_train[['Soil_Type10']])
table(data_train[['Slope']])
column_unique_df <- data.frame(column_name = character(0), second_frequency = list())
for(column in colnames(data_train)){
unique_vals <- table(data_train[[column]])
sorted_vals <- sort(unique_vals, decreasing = TRUE)
second_largest <- sorted_vals[2]
new_row <- data.frame(column_name = column, second_frequency = second_largest)
colnames(new_row) <- colnames(column_unique_df)
column_unique_df <- rbind(column_unique_df, new_row)
# for(i in seq_along(data_train[['colnames.data_train.']])){
#   if (data_train[['colnames.data_train.']][i] == column){
#     column_unique_df[['frequency']][i] <- unique_vals
#   }
# }
}
unique_vals <- table(data_train[['Slope']])
sorted_vals <- sort(unique_vals, decreasing = TRUE)
second_largest <- sorted_vals[2]
second_largest
sorted_vals
column_unique_df <- data.frame(column_name = character(0), second_frequency = list())
column_unique_df
column_unique_df <- data.frame(column_name = character(0), frequency = list())
column_unique_df
# column_unique_df$frequency <- NA
view(column_unique_df)
# column_unique_df$frequency <- NA
view(column_unique_df)
for(column in colnames(data_train)){
unique_vals <- table(data_train[[column]])
sorted_vals <- sort(unique_vals, decreasing = TRUE)
second_largest <- sorted_vals[2]
new_row <- data.frame(column_name = column, second_frequency = second_largest)
colnames(new_row) <- colnames(column_unique_df)
column_unique_df <- rbind(column_unique_df, new_row)
# for(i in seq_along(data_train[['colnames.data_train.']])){
#   if (data_train[['colnames.data_train.']][i] == column){
#     column_unique_df[['frequency']][i] <- unique_vals
#   }
# }
}
for(column in colnames(data_train)){
unique_vals <- table(data_train[[column]])
sorted_vals <- sort(unique_vals, decreasing = TRUE)
second_largest <- sorted_vals[2]
new_row <- data.frame(column_name = column, second_frequency = second_largest)
colnames(new_row) <- colnames(column_unique_df)
rownames(new_row) <- NULL
column_unique_df <- rbind(column_unique_df, new_row)
# for(i in seq_along(data_train[['colnames.data_train.']])){
#   if (data_train[['colnames.data_train.']][i] == column){
#     column_unique_df[['frequency']][i] <- unique_vals
#   }
# }
}
for(column in colnames(data_train)){
unique_vals <- table(data_train[[column]])
sorted_vals <- sort(unique_vals, decreasing = TRUE)
second_largest <- sorted_vals[2]
new_row <- data.frame(column_name = column, second_frequency = second_largest)
colnames(new_row) <- colnames(column_unique_df)
column_unique_df <- rbind(column_unique_df, new_row)
}
column_unique_df <- data.frame(column_name = character(0), frequency = list())
for(column in colnames(data_train)){
unique_vals <- table(data_train[[column]])
sorted_vals <- sort(unique_vals, decreasing = TRUE)
second_largest <- sorted_vals[2]
new_row <- data.frame(column_name = column, frequency = second_largest)
colnames(new_row) <- colnames(column_unique_df)
column_unique_df <- rbind(column_unique_df, new_row)
}
unique_vals <- table(data_train[['Slope']])
sorted_vals <- sort(unique_vals, decreasing = TRUE)
second_largest <- sorted_vals[2]
second_largest
second_largest <- sorted_vals[2][1]
second_largest
second_largest <- as.numeric(sorted_vals[2])[1]
second_largest
column_unique_df <- data.frame(column_name = character(0), frequency = list())
for(column in colnames(data_train)){
unique_vals <- table(data_train[[column]])
sorted_vals <- sort(unique_vals, decreasing = TRUE)
second_largest <- as.numeric(sorted_vals[2])[1]
new_row <- data.frame(column_name = column, frequency = second_largest)
colnames(new_row) <- colnames(column_unique_df)
column_unique_df <- rbind(column_unique_df, new_row)
}
column_unique_df
colnames(column_unique_df) <- c('column_name', 'second_freq')
column_unique_df
view(column_unique_df)
?step_nzv
table(data_train[['Elevation']])
sort(table(data_train[['Elevation']]), decreasing = TRUE)
sort(table(data_train[['Soil_Type34']]), decreasing = TRUE)
sort(table(data_train[['Horizontal_Distance_To_Roadways']]), decreasing = TRUE)
# Import Dataset:
data_train <- vroom("./data/train.csv") %>%
mutate(Cover_Type=factor(Cover_Type))# grab training data
ncol(data_train)
# view(data_train)
data_train$Cover_Type
rFormula <- Cover_Type ~ .
## For target encoding/Random Forests: ###
class_rf_recipe <- recipe(rFormula, data = data_train) %>% # set model formula and dataset
step_mutate_at(c(12:55), fn = factor) %>%
#step_other(all_nominal_predictors(), threshold = .001) %>%
# step_zv(all_predictors()) %>% # eliminate zero variance predictors %>%
step_nzv(freq_cut = 15070/50) %>%
step_lencode_glm(all_nominal_predictors(), outcome = vars(Cover_Type)) #%>%
prepped_recipe <- prep(class_rf_recipe) # preprocessing new data
baked_data1 <- bake(prepped_recipe, new_data = data_train)
unique(data_train$Soil_Type7)
## For target encoding/Random Forests: ###
class_rf_recipe <- recipe(rFormula, data = data_train) %>% # set model formula and dataset
step_mutate_at(c(12:55), fn = factor) %>%
#step_other(all_nominal_predictors(), threshold = .001) %>%
# step_zv(all_predictors()) %>% # eliminate zero variance predictors %>%
step_nzv(freq_cut = 15070/50) %>%
step_lencode_glm(all_nominal_predictors(), outcome = vars(Cover_Type)) #%>%
prepped_recipe <- prep(class_rf_recipe) # preprocessing new data
baked_data1 <- bake(prepped_recipe, new_data = data_train)
rFormula <- Cover_Type ~ .
## For target encoding/Random Forests: ###
class_rf_recipe <- recipe(rFormula, data = data_train) %>% # set model formula and dataset
step_mutate_at(c(12:55), fn = factor) %>%
#step_other(all_nominal_predictors(), threshold = .001) %>%
step_zv(all_predictors()) %>% # eliminate zero variance predictors
step_nzv(freq_cut = 15070/50) %>%
step_lencode_glm(all_nominal_predictors(), outcome = vars(Cover_Type)) #%>%
prepped_recipe <- prep(class_rf_recipe) # preprocessing new data
baked_data1 <- bake(prepped_recipe, new_data = data_train)
baked_data1
