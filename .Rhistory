mu.n <- 4
sigma2.n <- 0.5
# initializations for the Gibbs Sampling Algorithm
iters <- 10000
muf.save <- mun.save <- rep(0, iters)
muf.save[1] <- mu.f
mun.save[1] <- mu.n
sigma2f.save <- sigma2n.save <- rep(0, iters)
sigma2f.save[1] <- sigma2.f
sigma2n.save[1] <- sigma2.n
#Gibbs Sampling Algorithm
for(t in 2:iters){
# Full conditional of mu.f (update the value of the parameters)
lambdaf.p <- (tau2*sum(fiction) + sigma2.f*lambda)/(tau2*n.f + sigma2.f)
tau2f.p <- sigma2.f*tau2/(tau2*n.f + sigma2.f)
#sample a new value of mu.f
mu.f <- rnorm(1, lambdaf.p, sqrt(tau2f.p))
#save the value of mu.f
muf.save[t] <- mu.f
#Repeat for mu.n
lambdan.p <- (tau2*sum(nonfiction) + sigma2.n*lambda)/(tau2*n.n + sigma2.n)
tau2n.p <- sigma2.n*tau2/(tau2*n.n + sigma2.n)
mu.n <- rnorm(1, lambdan.p, sqrt(tau2n.p))
mun.save[t] <- mu.n
# full conditional of sigma2 (update the value of the parameters)
gammaf.p <- gamma + n.f/2
phif.p <- phi + sum((fiction - mu.f)^2 )/2
#sample new value of sigma2.f
sigma2.f <- rinvgamma(1, gammaf.p, phif.p)
#save the value of sigma2.f
sigma2f.save[t] <- sigma2.f
#Repeat for sigma2.n
gamman.p <- gamma + n.n/2
phin.p <- phi + sum((nonfiction - mu.n)^2 )/2
sigma2.n <- rinvgamma(1, gamman.p, phin.p)
sigma2n.save[t] <- sigma2.n
}
# Trace plots (decide if we need to throw out the first few values)
par(mfrow=c(1,2))
plot(muf.save, type='l')
plot(sigma2f.save, type='l')
plot(mun.save, type='l')
plot(sigma2n.save, type='l')
#throw out the first few values
burn <- 100
muf.use <- muf.save[-(1:burn)]
sigma2f.use <- sigma2f.save[-(1:burn)]
mun.use <- mun.save[-(1:burn)]
sigma2n.use <- sigma2n.save[-(1:burn)]
plot(muf.use, type='l')
plot(sigma2f.use, type='l')
plot(mun.use, type='l')
plot(sigma2n.use, type='l')
plot(density(muf.use), xlab=expression(mu), ylab="density", main=expression(pi(mu~"|"~data)), lwd=2)
lines(density(mun.use), col='gray', lwd=2)
legend("topleft", c("Fiction", "Nonfiction"), col=c("black", "gray"), lty=1, lwd=2)
# posterior distributions of sigma2.f and sigma2.n
plot(density(sigma2f.use), xlab=expression(sigma^2), ylab="density", main=expression(pi(sigma^2~"|"~data)), lwd=2)
lines(density(sigma2n.use), col='gray', lwd=2)
legend("topright", c("Fiction", "Nonfiction"), col=c("black", "gray"), lty=1, lwd=2)
mean(muf.use > mun.use)
#Given our data and prior knowledge, there is about a 30% chance that
#NYTimes Bestseller fiction books have a higher goodreads rating than nonfiction books
# Credible interval
diff <- muf.use - mun.use
plot(density(diff), xlab=expression(mu[f]-mu[n]), main="Posterior of Average Fiction vs Nonfiction")
abline(v=0, lty=2)
quantile(diff, c(.025, .975))
knitr::opts_chunk$set(echo = TRUE)
library(invgamma)
library(zoo)
# WE:
set.seed(1127)
####################
# # PRIOR PARAMETERS
####################
# # Prior parameters for mu:
lambda <- 0.7
tau2 <- 10^2 #relatively large
# # Prior parameters for sigma2:
gamma <- 2.001
phi <- 0.08
# #prior expected value of variance:
# phi/(gamma-1)
#
# # Plot the prior distributions to make sure they seem reasonable
# par(mfrow=c(1,2))
# curve(dnorm(x, lambda, sqrt(tau2)), xlim=c(-45, 45), ylab="prior density", main=expression(pi(mu)), xlab=expression(mu))
# curve(dinvgamma(x, gamma, phi), xlim=c(0, 1), ylab="prior density", main=expression(pi(sigma^2)), xlab=expression(sigma^2))
##############
# COLLECT DATA
##############
data1 <- c(0.66, 0.66, 0.67, 0.68, 0.68, 0.72, 0.73, 0.73, 0.73, 0.75, 0.75,
0.76, 0.77, 0.77, 0.77, 0.77,0.78, 0.78, 0.78, 0.79, 0.79, 0.79,
0.79, 0.8, 0.8,0.81, 0.82, 0.83, 0.83, 0.83, 0.84, 0.84,0.84, 0.85,
0.86, 0.86, 0.86, 0.87, 0.89, 0.91)
n <- length(data1)
#############################################
# POSTERIOR DISTRIBUTIONS: use Gibbs Sampling
#############################################
# Starting values:
# mu <- 0.77
mu <- mean(data1)
# sigma2 <- 0.07^2
sigma2 <- var(data1)
# initializations for the Gibbs Sampling Algorithm
iters <- 10000
mu.save <- rep(0, iters)
mu.save[1] <- mu
sigma2.save <- rep(0, iters)
sigma2.save[1] <- sigma2
#Gibbs Sampling Algorithm
for(t in 2:iters){
# Full conditional of mu (update the value of the parameters)
lambda.p <- (tau2*sum(data1) + sigma2*lambda)/(tau2*n + sigma2)
tau2.p <- sigma2*tau2/(tau2*n + sigma2)
#sample a new value of mu
mu <- rnorm(1, lambda.p, sqrt(tau2.p))
#save the value of mu
mu.save[t] <- mu
# full conditional of sigma2 (update the value of the parameters)
gamma.p <- gamma + n/2
phi.p <- phi + sum((data1 - mu)^2 )/2
#sample new value of sigma2
sigma2 <- rinvgamma(1, gamma.p, phi.p)
#save the value of sigma2
sigma2.save[t] <- sigma2
}
# # Find rolling mean to plot:
# mu.rollmean <- rollmean(mu.save, k = 90, fill = NA)
# # mu.save.sub <- mu.save[seq_along(mu.save) %% 2 != 0]
# mu.save.sub <- mu.save[1:4000]
# # Find rolling mean to plot sigma^2:
# s2.rollmean <- rollmean(sigma2.save, k = 90, fill = NA)
# # mu.save.sub <- mu.save[seq_along(mu.save) %% 2 != 0]
# s2.save.sub <- sigma2.save[1:4000]
# Trace plots (decide if we need to throw out the first few values):
# plot(mu.save.sub, type='l')
# lines(x = mu.rollmean, col = 'light green', type = 'l')
# plot(s2.save.sub, type='l')
# lines(x = s2.rollmean, col = 'light green', type = 'l')
par(mfrow=c(1,2))
plot(mu.save, type='l')
plot(sigma2.save, type='l')
#throw out the first few values
burn <- 200
mu.use <- mu.save[-(1:burn)]
sigma2.use <- sigma2.save[-(1:burn)]
plot(mu.use, type='l')
plot(sigma2.use, type='l')
#SUMMARIZE THE POSTERIOR DISTRIBUTION(S)
# posterior distribution of mu
#plot
plot(density(mu.use), xlab=expression(mu[W]), ylab="density", main=expression(pi(mu[W]~"|"~data)))
# #add prior
# curve(dnorm(x, lambda, sqrt(tau2)), lty=2, add=T)
#add legend
# legend("topright", c("Posterior"))
# posterior distribution of sigma2
par(mfrow=c(1,1))
plot(density(sigma2.use), xlab=expression(sigma[W]^2), main=expression(pi(sigma[W]^2~"|"~data)))
# #add prior
# curve(dinvgamma(x, gamma, phi), add=T, lty=2, xlim=c(0, 1))
#add legend
# legend("topright", c("Posterior"))
# EA:
# COLLECT DATA
data2 <- c(0.54, 0.57, 0.58, 0.61, 0.62, 0.62, 0.64, 0.64, 0.67, 0.68, 0.68,
0.68, 0.69, 0.69, 0.69, 0.7, 0.7, 0.71, 0.71, 0.72, 0.72, 0.73,
0.74, 0.74, 0.74, 0.74, 0.76, 0.77, 0.77, 0.78, 0.79, 0.8, 0.8,
0.8, 0.82, 0.82, 0.83, 0.85, 0.86, 0.87)
n2 <- length(data2)
# POSTERIOR DISTRIBUTIONS: use Gibbs Sampling
# Starting values:
# mu_ea <- 0.69
mu_ea <- mean(data2)
# sigma2_ea <- 0.07^2
sigma2_ea <- var(data2)
# initializations for the Gibbs Sampling Algorithm
iters <- 10000
mu_ea.save <- rep(0, iters)
mu_ea.save[1] <- mu_ea
sigma2_ea.save <- rep(0, iters)
sigma2_ea.save[1] <- sigma2_ea
#Gibbs Sampling Algorithm
for(t in 2:iters){
# Full conditional of mu (update the value of the parameters)
lambda.p <- (tau2*sum(data2) + sigma2_ea*lambda)/(tau2*n2 + sigma2_ea)
tau2.p <- sigma2_ea*tau2/(tau2*n2 + sigma2_ea)
#sample a new value of mu
mu_ea <- rnorm(1, lambda.p, sqrt(tau2.p))
#save the value of mu
mu_ea.save[t] <- mu_ea
# full conditional of sigma2 (update the value of the parameters)
gamma.p <- gamma + n2/2
phi.p <- phi + sum((data2 - mu_ea)^2 )/2
#sample new value of sigma2
sigma2_ea <- rinvgamma(1, gamma.p, phi.p)
#save the value of sigma2
sigma2_ea.save[t] <- sigma2_ea
}
# Trace plots (decide if we need to throw out the first few values)
par(mfrow=c(1,2))
plot(mu_ea.save, type='l')
plot(sigma2_ea.save, type='l')
#throw out the first few values
burn <- 200
mu_ea.use <- mu_ea.save[-(1:burn)]
sigma2_ea.use <- sigma2_ea.save[-(1:burn)]
plot(mu_ea.use, type='l')
plot(sigma2_ea.use, type='l')
#SUMMARIZE THE POSTERIOR DISTRIBUTION(S)
# posterior distribution of mu
#plot
plot(density(mu_ea.use), xlab=expression(mu[E]), ylab="density", main=expression(pi(mu[E]~"|"~data)))
# #add prior
# curve(dnorm(x, lambda, sqrt(tau2)), lty=2, add=T)
#add legend
#legend("topright", c("Posterior"))
# posterior distribution of sigma2
par(mfrow=c(1,1))
plot(density(sigma2_ea.use), xlab=expression(sigma[E]^2), main=expression(pi(sigma[E]^2~"|"~data)))
# #add prior
# curve(dinvgamma(x, gamma, phi), add=T, lty=2, xlim=c(0, 1))
#add legend
#legend("topright", c("Posterior"))
# Posterior Dist of the difference between mean AUROC for WE and EA:
# Credible interval
diff <- mu.use - mu_ea.use
plot(density(diff), xlab=expression(mu[W]-mu[E]), main=expression(~"Posterior of " ~mu[W] ~" - " ~mu[E]))
abline(v=0, lty=2)
quantile(diff, c(.025, .975))
# Posterior Dist of the difference between variance of AUROC for WE and EA:
# Credible interval
diff <- sigma2.use - sigma2_ea.use
plot(density(diff), xlab=expression(sigma[W]^2-sigma[E]^2), main=expression(~"Posterior of " ~sigma[W]^2 ~" - " ~sigma[E]^2))
abline(v=0, lty=2)
quantile(diff, c(.025, .975))
# WE:
# # PRIOR PARAMETERS
# # Prior parameters for mu:
lambda <- 0.7
tau2 <- 10^2 #relatively large
# # Prior parameters for sigma2:
gamma <- 2.001
phi <- 0.08
# #prior expected value of variance:
# phi/(gamma-1)
#
# # Plot the prior distributions to make sure they seem reasonable
# par(mfrow=c(1,2))
# curve(dnorm(x, lambda, sqrt(tau2)), xlim=c(-45, 45), ylab="prior density", main=expression(pi(mu)), xlab=expression(mu))
# curve(dinvgamma(x, gamma, phi), xlim=c(0, 1), ylab="prior density", main=expression(pi(sigma^2)), xlab=expression(sigma^2))
# COLLECT DATA
data1 <- c( 0.68, 0.71, 0.71, 0.73, 0.74, 0.74, 0.74, 0.75, 0.75, 0.75, 0.76,
0.76, 0.76, 0.76, 0.76, 0.77, 0.77, 0.78, 0.79, 0.79, 0.79, 0.8,
0.82, 0.82, 0.82, 0.83, 0.83, 0.83, 0.83, 0.84, 0.84, 0.84, 0.84,
0.84, 0.85, 0.85, 0.86, 0.87, 0.87, 0.89)
n <- length(data1)
# POSTERIOR DISTRIBUTIONS: use Gibbs Sampling
# Starting values:
mu <- 0.79
sigma2 <- 0.04^2
# initializations for the Gibbs Sampling Algorithm
iters <- 10000
mu.save <- rep(0, iters)
mu.save[1] <- mu
sigma2.save <- rep(0, iters)
sigma2.save[1] <- sigma2
#Gibbs Sampling Algorithm
for(t in 2:iters){
# Full conditional of mu (update the value of the parameters)
lambda.p <- (tau2*sum(data1) + sigma2*lambda)/(tau2*n + sigma2)
tau2.p <- sigma2*tau2/(tau2*n + sigma2)
#sample a new value of mu
mu <- rnorm(1, lambda.p, sqrt(tau2.p))
#save the value of mu
mu.save[t] <- mu
# full conditional of sigma2 (update the value of the parameters)
gamma.p <- gamma + n/2
phi.p <- phi + sum((data1 - mu)^2 )/2
#sample new value of sigma2
sigma2 <- rinvgamma(1, gamma.p, phi.p)
#save the value of sigma2
sigma2.save[t] <- sigma2
}
# Trace plots (decide if we need to throw out the first few values)
par(mfrow=c(1,2))
plot(mu.save, type='l')
plot(sigma2.save, type='l')
#throw out the first few values
burn <- 200
mu.use <- mu.save[-(1:burn)]
sigma2.use <- sigma2.save[-(1:burn)]
plot(mu.use, type='l')
plot(sigma2.use, type='l')
#SUMMARIZE THE POSTERIOR DISTRIBUTION(S)
# posterior distribution of mu
#plot
plot(density(mu.use), xlab=expression(mu), ylab="density", main=expression(pi(mu[W]~"|"~data)))
# #add prior
# curve(dnorm(x, lambda, sqrt(tau2)), lty=2, add=T)
#add legend
# legend("topright", c("Posterior"))
# posterior distribution of sigma2
par(mfrow=c(1,1))
plot(density(sigma2.use), xlab=expression(sigma^2), main=expression(pi(sigma[W]^2~"|"~data)))
# #add prior
# curve(dinvgamma(x, gamma, phi), add=T, lty=2, xlim=c(0, 1))
#add legend
# legend("topright", c("Posterior"))
knitr::opts_chunk$set(echo = TRUE)
library(invgamma)
library(zoo)
shape <- 0.5
rate <- 0.5
#collect data
data <- c(3, 3, 7, 1, 4, 6, 6, 7, 3, 1, 5, 5, 5, 3, 3, 0, 3, 1, 2, 2)
n <- length(data)
n
iters <- 10000 #number of iterations
lambda.save <- rep(0, iters) #initialize a vector to save the accepted values of the parameter
lambda <- 0.5 #starting value of parameter
n.accept <- 0 #tracker: how many times do we accept the proposed value?
s <- 0.3
#MCMC algorithm (metropolis random walk)
for(i in 1:iters){
lambda.dot <- rnorm(1, lambda, s)
if(lambda.dot <= 1 & lambda.dot >=0){
logr <- dpois(data, lambda.dot, log=T) + dgamma(data, shape, rate, log=T) - dpois(data, lambda.dot, log=T) - dgamma(data, shape, rate, log=T)
logu <- log(runif(1))
if(logu <= logr){
lambda <- lambda.dot
n.accept <- n.accept+1
}
}
lambda.save[i] <- lambda
}
lambda.dot
lambda
knitr::opts_chunk$set(echo = TRUE)
library(invgamma)
library(zoo)
# Rejection Sampling #
set.seed(1128)
# Proposed k value
k <- .25
#prior parameters
shape <- 0.5
rate <- 0.5
#collect data
data <- c(3, 3, 7, 1, 4, 6, 6, 7, 3, 1, 5, 5, 5, 3, 3, 0, 3, 1, 2, 2)
n <- length(data)
#Rejection sampling setup
iters <- 10000 #number of iterations
lambda.save <- NULL #initialize a variable to save the accepted values of the parameter
# Rejection Sampling
for(i in 1:iters){
lambda.dot <- runif(1)
r <- dpois(data, lambda = lambda.dot, log=T)*dgamma(lambda.dot, shape = shape, rate = rate, log=T)/(k*dunif(lambda.dot, 0, 1))
# Four ways to accept or reject the value of theta.dot with probability r...
## First:
accept <- sample(c(T, F), 1, prob=c(r, 1-r))
if(accept==T){
lambda.save <- c(lambda.save, lambda.dot)}
## Second:
# accept <- rbinom(n=1, size=1, prob=r)
# if(accept==1){theta.save <- c(theta.save, theta.dot)}
## Third:
# u <- runif(1)
# if(u <= r){theta.save <- c(theta.save, theta.dot)}
## Fourth (same as third, but on log scale; best way computationally):
# logr <- dbinom(y, n, theta.dot, log=T) + dbeta(theta.dot, alpha, beta, log=T) - log(k) - dunif(theta.dot, 0, 1, log=T)
# logu <- log(runif(1))
# if(logu <= logr){theta.save <- c(theta.save, theta.dot)}
}
?sample
# Rejection Sampling #
set.seed(1128)
# Proposed k value
k <- .25
#prior parameters
shape <- 0.5
rate <- 0.5
#collect data
data <- c(3, 3, 7, 1, 4, 6, 6, 7, 3, 1, 5, 5, 5, 3, 3, 0, 3, 1, 2, 2)
n <- length(data)
#Rejection sampling setup
iters <- 10000 #number of iterations
lambda.save <- NULL #initialize a variable to save the accepted values of the parameter
# Rejection Sampling
for(i in 1:iters){
lambda.dot <- runif(1)
r <- dpois(data, lambda = lambda.dot, log=T)*dgamma(lambda.dot, shape = shape, rate = rate, log=T)/(k*dunif(lambda.dot, 0, 1))
# Four ways to accept or reject the value of theta.dot with probability r...
## First:
accept <- sample(data, 1, prob=r)
if(accept==T){
lambda.save <- c(lambda.save, lambda.dot)}
## Second:
# accept <- rbinom(n=1, size=1, prob=r)
# if(accept==1){theta.save <- c(theta.save, theta.dot)}
## Third:
# u <- runif(1)
# if(u <= r){theta.save <- c(theta.save, theta.dot)}
## Fourth (same as third, but on log scale; best way computationally):
# logr <- dbinom(y, n, theta.dot, log=T) + dbeta(theta.dot, alpha, beta, log=T) - log(k) - dunif(theta.dot, 0, 1, log=T)
# logu <- log(runif(1))
# if(logu <= logr){theta.save <- c(theta.save, theta.dot)}
}
# Rejection Sampling #
set.seed(1128)
# Proposed k value
k <- .25
#prior parameters
shape <- 0.5
rate <- 0.5
#collect data
data <- c(3, 3, 7, 1, 4, 6, 6, 7, 3, 1, 5, 5, 5, 3, 3, 0, 3, 1, 2, 2)
n <- length(data)
#Rejection sampling setup
iters <- 10000 #number of iterations
lambda.save <- NULL #initialize a variable to save the accepted values of the parameter
# Rejection Sampling
for(i in 1:iters){
lambda.dot <- runif(1)
r <- dpois(data, lambda = lambda.dot, log=T)*dgamma(lambda.dot, shape = shape, rate = rate, log=T)/(k*dunif(lambda.dot, 0, 1))
# Four ways to accept or reject the value of theta.dot with probability r...
## First:
accept <- sample(data, 1, prob=lambda.dot)
if(accept==T){
lambda.save <- c(lambda.save, lambda.dot)}
## Second:
# accept <- rbinom(n=1, size=1, prob=r)
# if(accept==1){theta.save <- c(theta.save, theta.dot)}
## Third:
# u <- runif(1)
# if(u <= r){theta.save <- c(theta.save, theta.dot)}
## Fourth (same as third, but on log scale; best way computationally):
# logr <- dbinom(y, n, theta.dot, log=T) + dbeta(theta.dot, alpha, beta, log=T) - log(k) - dunif(theta.dot, 0, 1, log=T)
# logu <- log(runif(1))
# if(logu <= logr){theta.save <- c(theta.save, theta.dot)}
}
gc()
####### Load Libraries #######
#install.packages('tidyverse')
library(tidyverse)
#install.packages('tidymodels')
library(tidymodels)
#install.packages('DataExplorer')
#install.packages("poissonreg")
# library(poissonreg)
#install.packages("glmnet")
library(glmnet)
#library(patchwork)
# install.packages("rpart")
#install.packages('ranger')
library(ranger)
#install.packages('stacks')
library(stacks)
#install.packages('vroom')
library(vroom)
#install.packages('parsnip')
library(parsnip)
# install.packages('dbarts')
# library(dbarts)
#install.packages('embed')
library(embed)
library(themis)
library(ggplot2)
library(parsnip)
####################
data_train <- vroom("./data/train.csv")
setwd("~/byu_fall_2023/Stat_348/STAT348/ForestCoverType/data")
####### Load Libraries #######
#install.packages('tidyverse')
library(tidyverse)
#install.packages('tidymodels')
library(tidymodels)
#install.packages('DataExplorer')
#install.packages("poissonreg")
# library(poissonreg)
#install.packages("glmnet")
library(glmnet)
#library(patchwork)
# install.packages("rpart")
#install.packages('ranger')
library(ranger)
#install.packages('stacks')
library(stacks)
#install.packages('vroom')
library(vroom)
#install.packages('parsnip')
library(parsnip)
# install.packages('dbarts')
# library(dbarts)
#install.packages('embed')
library(embed)
library(themis)
library(ggplot2)
library(parsnip)
data_train <- vroom("./data/train.csv")
setwd("~/byu_fall_2023/Stat_348/STAT348/ForestCoverType")
data_train <- vroom("./data/train.csv")
view(data_train)
cor_matrix <- cor(data_train)
# Create corr heatmap
heatmap(cor_matrix,
col = colorRampPalette(c('blue', 'white', 'green'))(100),
margins = c(5, 5),
Rowv = NA,
Colv = NA)
